{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nhMno-Z2Aco8"
      },
      "outputs": [],
      "source": [
        "# The code to run the CNN-LSTM-Attention-Residual hybrid model\n",
        "\n",
        "# Import libraries\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.metrics import classification_report, precision_recall_curve, roc_auc_score, f1_score, auc, matthews_corrcoef\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.optim import Adam\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Parameters\n",
        "SEQ_LENGTH = 300\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 25\n",
        "PATIENCE = 7  # Early stopping patience\n",
        "DATA_FOLDER = '/kaggle/input/algae-dataset'  # Input data\n",
        "CHECKPOINT_PATH_WORKING = '/kaggle/working/Algae_Final_checkpoint.pth'  # Path for checkpoint as input\n",
        "CHECKPOINT_PATH_INPUT = '/kaggle/input/Algae_checkpoint_fold4_epoch24.pth'\n",
        "K_FOLDS = 5  # Number of folds for cross-validation\n",
        "\n",
        "# EarlyStopping class\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=7, delta=0, verbose=False):\n",
        "        self.patience = patience\n",
        "        self.delta = delta\n",
        "        self.verbose = verbose\n",
        "        self.best_loss = None\n",
        "        self.counter = 0\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, val_loss):\n",
        "        if self.best_loss is None or val_loss < self.best_loss - self.delta:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.verbose:\n",
        "                print(f\"EarlyStopping: No improvement in validation loss for {self.counter} epochs.\")\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "\n",
        "# One-hot encoding function\n",
        "def one_hot_encode(sequence, seq_length=SEQ_LENGTH):\n",
        "    nucleotide_map = {'A': [1, 0, 0, 0, 0], 'T': [0, 1, 0, 0, 0],\n",
        "                      'C': [0, 0, 1, 0, 0], 'G': [0, 0, 0, 1, 0],\n",
        "                      'N': [0, 0, 0, 0, 1]}\n",
        "    sequence = sequence.upper().ljust(seq_length, 'N')[:seq_length]\n",
        "    return np.array([nucleotide_map.get(char, [0, 0, 0, 0, 1]) for char in sequence])\n",
        "\n",
        "# Dataset class\n",
        "class SequenceDataset(Dataset):\n",
        "    def __init__(self, sequences, labels):\n",
        "        self.sequences = sequences\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sequence = torch.tensor(self.sequences[idx], dtype=torch.float32)\n",
        "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        return sequence, label\n",
        "\n",
        "# Load data function\n",
        "def load_data(data_folder):\n",
        "    sequences = []\n",
        "    labels = []\n",
        "    class_names = []\n",
        "    for idx, file_name in enumerate(os.listdir(data_folder)):\n",
        "        if file_name.endswith('.csv'):\n",
        "            file_path = os.path.join(data_folder, file_name)\n",
        "            data = pd.read_csv(file_path, header=None)\n",
        "            sequences.extend(data[0].tolist())\n",
        "            labels.extend([idx] * len(data))\n",
        "            class_names.append(os.path.splitext(file_name)[0])  # Strip the '.csv' extension\n",
        "    one_hot_sequences = np.array([one_hot_encode(seq) for seq in sequences])\n",
        "    return one_hot_sequences, np.array(labels), class_names\n",
        "\n",
        "# Self-Attention Mechanism\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.attention = nn.MultiheadAttention(embed_dim, num_heads)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(1, 0, 2)  # MultiheadAttention expects (seq_len, batch, embed_dim)\n",
        "        attn_output, _ = self.attention(x, x, x)\n",
        "        return attn_output.permute(1, 0, 2)  # Return to (batch, seq_len, embed_dim)\n",
        "\n",
        "# Multi-kernel CNN with Residual Connections\n",
        "class MultiKernelCNN(nn.Module):\n",
        "    def __init__(self, input_channels, output_channels, use_multi_kernel=True):\n",
        "        super(MultiKernelCNN, self).__init__()\n",
        "        self.use_multi_kernel = use_multi_kernel\n",
        "        if use_multi_kernel:\n",
        "            # Multiple kernel sizes for the second layer\n",
        "            self.conv3 = nn.Conv1d(input_channels, output_channels, kernel_size=3, padding=1)\n",
        "            self.conv5 = nn.Conv1d(input_channels, output_channels, kernel_size=5, padding=2)\n",
        "            self.conv7 = nn.Conv1d(input_channels, output_channels, kernel_size=7, padding=3)\n",
        "        else:\n",
        "            # Single kernel size for the first layer\n",
        "            self.conv3 = nn.Conv1d(input_channels, output_channels, kernel_size=3, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.residual = nn.Conv1d(input_channels, output_channels * (3 if use_multi_kernel else 1), kernel_size=1)\n",
        "        self.dropout = nn.Dropout(0.5)  # Increased dropout\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = self.residual(x)\n",
        "        if self.use_multi_kernel:\n",
        "            # Apply multiple kernels for the second layer\n",
        "            x1 = self.relu(self.conv3(x))\n",
        "            x2 = self.relu(self.conv5(x))\n",
        "            x3 = self.relu(self.conv7(x))\n",
        "            x = torch.cat((x1, x2, x3), dim=1)\n",
        "        else:\n",
        "            # Apply single kernel for the first layer\n",
        "            x = self.relu(self.conv3(x))\n",
        "        x = self.pool(x)\n",
        "        x = self.dropout(x)\n",
        "        if x.shape != residual.shape:\n",
        "            residual = residual[:, :, :x.shape[2]]\n",
        "        x = x + residual\n",
        "        return x\n",
        "\n",
        "# CNN Attention Mechanism\n",
        "class CNN_Attention(nn.Module):\n",
        "    def __init__(self, channel_dim):\n",
        "        super(CNN_Attention, self).__init__()\n",
        "        self.channel_dim = channel_dim\n",
        "        self.query = nn.Conv1d(channel_dim, channel_dim // 8, kernel_size=1)\n",
        "        self.key = nn.Conv1d(channel_dim, channel_dim // 8, kernel_size=1)\n",
        "        self.value = nn.Conv1d(channel_dim, channel_dim, kernel_size=1)\n",
        "        self.gamma = nn.Parameter(torch.zeros(1))  # Learnable scaling factor\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, channels, seq_len = x.size()\n",
        "        # Compute query, key, and value\n",
        "        query = self.query(x).view(batch_size, -1, seq_len)  # (batch, channels//8, seq_len)\n",
        "        key = self.key(x).view(batch_size, -1, seq_len)  # (batch, channels//8, seq_len)\n",
        "        value = self.value(x).view(batch_size, -1, seq_len)  # (batch, channels, seq_len)\n",
        "        # Compute attention scores\n",
        "        attention_scores = torch.bmm(query.permute(0, 2, 1), key)  # (batch, seq_len, seq_len)\n",
        "        attention_scores = F.softmax(attention_scores, dim=-1)\n",
        "        # Apply attention to value\n",
        "        out = torch.bmm(value, attention_scores.permute(0, 2, 1))  # (batch, channels, seq_len)\n",
        "        out = self.gamma * out + x  # Residual connection\n",
        "        return out\n",
        "\n",
        "# CNN-LSTM Hybrid Model with Self-Attention\n",
        "class CNN_LSTM_Model(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(CNN_LSTM_Model, self).__init__()\n",
        "        # First CNN layer: single kernel size of 3\n",
        "        self.multi_kernel_cnn1 = MultiKernelCNN(input_channels=5, output_channels=128, use_multi_kernel=False)\n",
        "        self.bn1 = nn.BatchNorm1d(128)  # BatchNorm after first CNN layer\n",
        "        # Second CNN layer: multiple kernel sizes (3, 5, 7)\n",
        "        self.multi_kernel_cnn2 = MultiKernelCNN(input_channels=128, output_channels=256, use_multi_kernel=True)\n",
        "        self.cnn_attention2 = CNN_Attention(channel_dim=256 * 3)  # CNN Attention after second CNN layer\n",
        "        self.bn2 = nn.BatchNorm1d(256 * 3)  # BatchNorm after second CNN layer\n",
        "        self.residual = nn.Conv1d(in_channels=5, out_channels=256 * 3, kernel_size=1)\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        # LSTM layer with BatchNorm\n",
        "        self.lstm = nn.LSTM(input_size=256 * 3, hidden_size=256, batch_first=True, bidirectional=True)\n",
        "        self.bn_lstm = nn.BatchNorm1d(256 * 2)  # BatchNorm after LSTM (output is 256 * 2 due to bidirectional)\n",
        "        self.self_attention = SelfAttention(embed_dim=256 * 2, num_heads=4)\n",
        "        self.dropout_lstm = nn.Dropout(0.3)  # Reduced dropout rate\n",
        "        # Fully connected layers with BatchNorm\n",
        "        self.fc1 = nn.Linear(256 * 2, 256)\n",
        "        self.bn_fc1 = nn.BatchNorm1d(256)  # BatchNorm after first FC layer\n",
        "        self.fc2 = nn.Linear(256, 512)\n",
        "        self.bn_fc2 = nn.BatchNorm1d(512)  # BatchNorm after second FC layer\n",
        "        self.fc3 = nn.Linear(512, num_classes)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout_fc = nn.Dropout(0.3)  # Reduced dropout rate\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 1)  # (batch, channels, seq_len)\n",
        "        residual = self.residual(x)\n",
        "        # First CNN layer: single kernel size of 3\n",
        "        x = self.multi_kernel_cnn1(x)\n",
        "        x = self.bn1(x)  # BatchNorm after first CNN layer\n",
        "        # Second CNN layer: multiple kernel sizes (3, 5, 7)\n",
        "        x = self.multi_kernel_cnn2(x)\n",
        "        x = self.cnn_attention2(x)  # Apply CNN Attention\n",
        "        x = self.bn2(x)  # BatchNorm after second CNN layer\n",
        "        x = self.pool(x)\n",
        "        x = self.dropout_fc(x)\n",
        "        if x.shape != residual.shape:\n",
        "            residual = residual[:, :, :x.shape[2]]\n",
        "        x = x + residual\n",
        "        x = x.permute(0, 2, 1)  # (batch, seq_len, channels)\n",
        "        # LSTM layer\n",
        "        lstm_out, (hn, _) = self.lstm(x)\n",
        "        lstm_out = lstm_out.permute(0, 2, 1)  # (batch, hidden_dim, seq_len) for BatchNorm\n",
        "        lstm_out = self.bn_lstm(lstm_out)  # BatchNorm after LSTM\n",
        "        lstm_out = lstm_out.permute(0, 2, 1)  # (batch, seq_len, hidden_dim)\n",
        "        lstm_out = self.dropout_lstm(lstm_out)\n",
        "        # Self-Attention\n",
        "        attn_out = self.self_attention(lstm_out)\n",
        "        context_vector = torch.mean(attn_out, dim=1)  # (batch, hidden_dim)\n",
        "        # Fully connected layers\n",
        "        x = self.fc1(context_vector)\n",
        "        x = self.bn_fc1(x)  # BatchNorm after first FC layer\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout_fc(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.bn_fc2(x)  # BatchNorm after second FC layer\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout_fc(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Load data\n",
        "sequences, labels, class_names = load_data(DATA_FOLDER)\n",
        "\n",
        "# After loading data\n",
        "print(f\"Sequences shape: {sequences.shape}, Labels shape: {labels.shape}\")\n",
        "print(f\"Class distribution: {np.bincount(labels)}\")\n",
        "\n",
        "# Split data into train and test sets (80% train, 20% test)\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
        "    sequences, labels, test_size=0.2, random_state=42, stratify=labels\n",
        ")\n",
        "\n",
        "# Debug: Check shapes after train-test split\n",
        "print(f\"X_train_val shape: {X_train_val.shape}, y_train_val shape: {y_train_val.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
        "print(f\"Class distribution in training set: {np.bincount(y_train_val)}\")\n",
        "print(f\"Class distribution in test set: {np.bincount(y_test)}\")\n",
        "\n",
        "# Initialize StratifiedKFold for cross-validation on the train set\n",
        "skf = StratifiedKFold(n_splits=K_FOLDS, shuffle=True, random_state=42)\n",
        "\n",
        "# Define device (CPU or GPU) at the beginning of the script\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Check for existing checkpoints\n",
        "if os.path.exists(CHECKPOINT_PATH_INPUT):\n",
        "    checkpoint = torch.load(CHECKPOINT_PATH_INPUT, map_location=device, weights_only=True)\n",
        "    print(f\"Resuming training from checkpoint: {CHECKPOINT_PATH_INPUT}\")\n",
        "    checkpoint_fold = int(CHECKPOINT_PATH_INPUT.split('fold')[-1].split('_')[0])  # Extract fold number from checkpoint path\n",
        "    checkpoint_epoch = checkpoint['epoch']  # Extract epoch number from checkpoint\n",
        "    print(f\"Checkpoint corresponds to Fold {checkpoint_fold}, Epoch {checkpoint_epoch}\")\n",
        "else:\n",
        "    checkpoint = None\n",
        "    checkpoint_fold = -1  # No checkpoint\n",
        "    checkpoint_epoch = -1\n",
        "    print(\"No checkpoint found. Starting training from scratch.\")\n",
        "\n",
        "\n",
        "# Cross-validation loop\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(X_train_val, y_train_val)):\n",
        "    fold_number = fold + 1  # Fold numbers start from 1\n",
        "\n",
        "    # Skip folds that were already completed\n",
        "    if checkpoint_fold != -1 and fold_number < checkpoint_fold:\n",
        "        print(f\"Skipping Fold {fold_number} (already completed).\", flush=True)\n",
        "        continue\n",
        "\n",
        "    print(f\"Fold {fold_number}/{K_FOLDS}\", flush=True)\n",
        "    X_train, X_val = X_train_val[train_idx], X_train_val[val_idx]\n",
        "    y_train, y_val = y_train_val[train_idx], y_train_val[val_idx]\n",
        "\n",
        "    # Debug: Check shapes for this fold\n",
        "    print(f\"X_train shape (fold {fold_number}): {X_train.shape}, y_train shape: {y_train.shape}\", flush=True)\n",
        "    print(f\"X_val shape (fold {fold_number}): {X_val.shape}, y_val shape: {y_val.shape}\", flush=True)\n",
        "\n",
        "    # Create datasets and dataloaders\n",
        "    train_dataset = SequenceDataset(X_train, y_train)\n",
        "    val_dataset = SequenceDataset(X_val, y_val)\n",
        "\n",
        "    # Initialize DataLoader for training and validation\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "\n",
        "    # Initialize model, loss, optimizer, and scheduler\n",
        "    model = CNN_LSTM_Model(num_classes=len(class_names)).to(device)\n",
        "    criterion = nn.CrossEntropyLoss().to(device)\n",
        "    optimizer = Adam(model.parameters(), lr=0.001, weight_decay=1e-5)  # Increased initial learning rate\n",
        "    scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=1e-6)  # Cosine annealing with warm restarts\n",
        "\n",
        "    # Load checkpoint if this is the fold we need to resume\n",
        "    if checkpoint_fold != -1 and fold_number == checkpoint_fold:\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        epoch_start = checkpoint_epoch + 1  # Resume from the next epoch\n",
        "        print(f\"Resuming training for Fold {fold_number} from epoch {epoch_start}\", flush=True)\n",
        "    else:\n",
        "        epoch_start = 0  # Start from scratch for other folds\n",
        "        print(f\"Starting training for Fold {fold_number} from scratch.\", flush=True)\n",
        "\n",
        "    # Early stopping with increased patience\n",
        "    early_stopping = EarlyStopping(patience=7, verbose=True)\n",
        "\n",
        "    # Mixed precision training (only if CUDA is available)\n",
        "    scaler = torch.amp.GradScaler(enabled=device.type == 'cuda')  # Corrected initialization\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(epoch_start, EPOCHS):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for sequences, labels in train_loader:\n",
        "            sequences, labels = sequences.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Mixed precision training\n",
        "            with torch.amp.autocast(device_type=device.type, enabled=device.type == 'cuda'):  # Corrected device_type\n",
        "                outputs = model(sequences)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for sequences, labels in val_loader:\n",
        "                sequences, labels = sequences.to(device), labels.to(device)\n",
        "                outputs = model(sequences)\n",
        "                loss = criterion(outputs, labels)  # Calculate loss\n",
        "                val_loss += loss.item()  # Accumulate validation loss\n",
        "\n",
        "        val_loss /= len(val_loader)  # Average validation loss\n",
        "        scheduler.step(val_loss)  # Update learning rate based on validation loss\n",
        "\n",
        "        # Print learning rate\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "        print(f\"Epoch {epoch+1}/{EPOCHS}, Train Loss: {total_loss/len(train_loader):.4f}, Val Loss: {val_loss:.4f}, LR: {current_lr:.6f}\", flush=True)\n",
        "\n",
        "        # Save model checkpoint every 2 epochs\n",
        "        if (epoch + 1) % 2 == 0:  # Checkpoint every 2 epochs\n",
        "            checkpoint_path = f\"/kaggle/working/Algae_checkpoint_fold{fold_number}_epoch{epoch+1}.pth\"\n",
        "            torch.save({\n",
        "                'epoch': epoch + 1,  # Save the current epoch\n",
        "                'model_state_dict': model.state_dict(),  # Save model weights\n",
        "                'optimizer_state_dict': optimizer.state_dict(),  # Save optimizer state\n",
        "                'val_loss': val_loss,  # Save validation loss\n",
        "            }, checkpoint_path)\n",
        "            print(f\"Checkpoint saved at {checkpoint_path}\", flush=True)\n",
        "\n",
        "        # Early stopping check\n",
        "        early_stopping(val_loss)\n",
        "        if early_stopping.early_stop:\n",
        "            print(\"Early stopping triggered. Stopping training.\", flush=True)\n",
        "            break\n",
        "\n",
        "\n",
        "\n",
        "    # Evaluate on the validation set for this fold\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_probs = []\n",
        "    with torch.no_grad():\n",
        "        for sequences, labels in val_loader:\n",
        "            sequences, labels = sequences.to(device), labels.to(device)\n",
        "            outputs = model(sequences)\n",
        "            probs = F.softmax(outputs, dim=1)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_probs.extend(probs.cpu().numpy())\n",
        "\n",
        "    # Calculate advanced metrics\n",
        "    roc_auc = roc_auc_score(all_labels, np.array(all_probs)[:, 1])  # Use probabilities of the positive class\n",
        "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "    mcc = matthews_corrcoef(all_labels, all_preds)  # Matthews Correlation Coefficient\n",
        "\n",
        "    # Precision-Recall curve (for binary classification, adjust if needed)\n",
        "    precision, recall, _ = precision_recall_curve(all_labels, np.array(all_probs)[:, 1], pos_label=1)\n",
        "    pr_auc = auc(recall, precision)\n",
        "\n",
        "    print(f\"Validation Set Results for Fold {fold_number}:\")\n",
        "    print(classification_report(all_labels, all_preds, target_names=class_names, digits=4))\n",
        "    print(f\"ROC-AUC: {roc_auc:.4f}, F1-Score: {f1:.4f}, MCC: {mcc:.4f}\")\n",
        "    print(f\"Precision-Recall AUC: {pr_auc:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "# Create test dataset and dataloader\n",
        "test_dataset = SequenceDataset(X_test, y_test)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "\n",
        "# Evaluate on the test set\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "all_probs = []\n",
        "with torch.no_grad():\n",
        "    for sequences, labels in test_loader:\n",
        "        sequences, labels = sequences.to(device), labels.to(device)\n",
        "        outputs = model(sequences)\n",
        "        probs = F.softmax(outputs, dim=1)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "        all_probs.extend(probs.cpu().numpy())\n",
        "\n",
        "# Calculate advanced metrics for the test set\n",
        "roc_auc = roc_auc_score(all_labels, np.array(all_probs)[:, 1])  # Use probabilities of the positive class\n",
        "f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "mcc = matthews_corrcoef(all_labels, all_preds)  # Matthews Correlation Coefficient\n",
        "\n",
        "# Precision-Recall curve (for binary classification, adjust if needed)\n",
        "precision, recall, _ = precision_recall_curve(all_labels, np.array(all_probs)[:, 1], pos_label=1)\n",
        "pr_auc = auc(recall, precision)\n",
        "\n",
        "print(\"Test Set Results:\")\n",
        "print(classification_report(all_labels, all_preds, target_names=class_names, digits=4))\n",
        "print(f\"ROC-AUC: {roc_auc:.4f}, F1-Score: {f1:.4f}, MCC: {mcc:.4f}\")\n",
        "print(f\"Precision-Recall AUC: {pr_auc:.4f}\")\n",
        "\n",
        "# Save the final model\n",
        "final_model_path = \"/kaggle/working/Final_Algae_model.pth\"\n",
        "torch.save(model.state_dict(), final_model_path)\n",
        "print(f\"Final model saved at {final_model_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The code to have the Confusion matrix\n",
        "\n",
        "# Import libraries\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, precision_recall_curve, roc_auc_score, f1_score, auc, matthews_corrcoef\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Parameters\n",
        "SEQ_LENGTH = 270\n",
        "BATCH_SIZE = 64\n",
        "DATA_FOLDER = '//kaggle/input/plants algae dataset (270nt)'\n",
        "FINAL_MODEL_PATH = '/kaggle/input/Plant-Algae(270nt)_Final checkpoint.pth'\n",
        "\n",
        "# One-hot encoding function\n",
        "def one_hot_encode(sequence, seq_length=SEQ_LENGTH):\n",
        "    nucleotide_map = {'A': [1, 0, 0, 0, 0], 'T': [0, 1, 0, 0, 0],\n",
        "                      'C': [0, 0, 1, 0, 0], 'G': [0, 0, 0, 1, 0],\n",
        "                      'N': [0, 0, 0, 0, 1]}\n",
        "    sequence = sequence.upper().ljust(seq_length, 'N')[:seq_length]\n",
        "    return np.array([nucleotide_map.get(char, [0, 0, 0, 0, 1]) for char in sequence])\n",
        "\n",
        "# Dataset class\n",
        "class SequenceDataset(Dataset):\n",
        "    def __init__(self, sequences, labels):\n",
        "        self.sequences = sequences\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sequence = torch.tensor(self.sequences[idx], dtype=torch.float32)\n",
        "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        return sequence, label\n",
        "\n",
        "# Load data function\n",
        "def load_data(data_folder):\n",
        "    sequences = []\n",
        "    labels = []\n",
        "    class_names = []\n",
        "    for idx, file_name in enumerate(os.listdir(data_folder)):\n",
        "        if file_name.endswith('.csv'):\n",
        "            file_path = os.path.join(data_folder, file_name)\n",
        "            data = pd.read_csv(file_path, header=None)\n",
        "            sequences.extend(data[0].tolist())\n",
        "            labels.extend([idx] * len(data))\n",
        "            class_names.append(os.path.splitext(file_name)[0])\n",
        "    one_hot_sequences = np.array([one_hot_encode(seq) for seq in sequences])\n",
        "    return one_hot_sequences, np.array(labels), class_names\n",
        "\n",
        "# Self-Attention Mechanism\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.attention = nn.MultiheadAttention(embed_dim, num_heads)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(1, 0, 2)\n",
        "        attn_output, _ = self.attention(x, x, x)\n",
        "        return attn_output.permute(1, 0, 2)\n",
        "\n",
        "# Multi-kernel CNN with Residual Connections\n",
        "class MultiKernelCNN(nn.Module):\n",
        "    def __init__(self, input_channels, output_channels, use_multi_kernel=True):\n",
        "        super(MultiKernelCNN, self).__init__()\n",
        "        self.use_multi_kernel = use_multi_kernel\n",
        "        if use_multi_kernel:\n",
        "            self.conv3 = nn.Conv1d(input_channels, output_channels, kernel_size=3, padding=1)\n",
        "            self.conv5 = nn.Conv1d(input_channels, output_channels, kernel_size=5, padding=2)\n",
        "            self.conv7 = nn.Conv1d(input_channels, output_channels, kernel_size=7, padding=3)\n",
        "        else:\n",
        "            self.conv3 = nn.Conv1d(input_channels, output_channels, kernel_size=3, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.residual = nn.Conv1d(input_channels, output_channels * (3 if use_multi_kernel else 1), kernel_size=1)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = self.residual(x)\n",
        "        if self.use_multi_kernel:\n",
        "            x1 = self.relu(self.conv3(x))\n",
        "            x2 = self.relu(self.conv5(x))\n",
        "            x3 = self.relu(self.conv7(x))\n",
        "            x = torch.cat((x1, x2, x3), dim=1)\n",
        "        else:\n",
        "            x = self.relu(self.conv3(x))\n",
        "        x = self.pool(x)\n",
        "        x = self.dropout(x)\n",
        "        if x.shape != residual.shape:\n",
        "            residual = residual[:, :, :x.shape[2]]\n",
        "        x = x + residual\n",
        "        return x\n",
        "\n",
        "# CNN Attention Mechanism\n",
        "class CNN_Attention(nn.Module):\n",
        "    def __init__(self, channel_dim):\n",
        "        super(CNN_Attention, self).__init__()\n",
        "        self.channel_dim = channel_dim\n",
        "        self.query = nn.Conv1d(channel_dim, channel_dim // 8, kernel_size=1)\n",
        "        self.key = nn.Conv1d(channel_dim, channel_dim // 8, kernel_size=1)\n",
        "        self.value = nn.Conv1d(channel_dim, channel_dim, kernel_size=1)\n",
        "        self.gamma = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, channels, seq_len = x.size()\n",
        "        query = self.query(x).view(batch_size, -1, seq_len)\n",
        "        key = self.key(x).view(batch_size, -1, seq_len)\n",
        "        value = self.value(x).view(batch_size, -1, seq_len)\n",
        "        attention_scores = torch.bmm(query.permute(0, 2, 1), key)\n",
        "        attention_scores = F.softmax(attention_scores, dim=-1)\n",
        "        out = torch.bmm(value, attention_scores.permute(0, 2, 1))\n",
        "        out = self.gamma * out + x\n",
        "        return out\n",
        "\n",
        "# CNN-LSTM Hybrid Model with Self-Attention\n",
        "class CNN_LSTM_Model(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(CNN_LSTM_Model, self).__init__()\n",
        "        self.multi_kernel_cnn1 = MultiKernelCNN(input_channels=5, output_channels=128, use_multi_kernel=False)\n",
        "        self.bn1 = nn.BatchNorm1d(128)\n",
        "        self.multi_kernel_cnn2 = MultiKernelCNN(input_channels=128, output_channels=256, use_multi_kernel=True)\n",
        "        self.cnn_attention2 = CNN_Attention(channel_dim=256 * 3)\n",
        "        self.bn2 = nn.BatchNorm1d(256 * 3)\n",
        "        self.residual = nn.Conv1d(in_channels=5, out_channels=256 * 3, kernel_size=1)\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.lstm = nn.LSTM(input_size=256 * 3, hidden_size=256, batch_first=True, bidirectional=True)\n",
        "        self.bn_lstm = nn.BatchNorm1d(256 * 2)\n",
        "        self.self_attention = SelfAttention(embed_dim=256 * 2, num_heads=4)\n",
        "        self.dropout_lstm = nn.Dropout(0.3)\n",
        "        self.fc1 = nn.Linear(256 * 2, 256)\n",
        "        self.bn_fc1 = nn.BatchNorm1d(256)\n",
        "        self.fc2 = nn.Linear(256, 512)\n",
        "        self.bn_fc2 = nn.BatchNorm1d(512)\n",
        "        self.fc3 = nn.Linear(512, num_classes)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout_fc = nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 1)\n",
        "        residual = self.residual(x)\n",
        "        x = self.multi_kernel_cnn1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.multi_kernel_cnn2(x)\n",
        "        x = self.cnn_attention2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.dropout_fc(x)\n",
        "        if x.shape != residual.shape:\n",
        "            residual = residual[:, :, :x.shape[2]]\n",
        "        x = x + residual\n",
        "        x = x.permute(0, 2, 1)\n",
        "        lstm_out, (hn, _) = self.lstm(x)\n",
        "        lstm_out = lstm_out.permute(0, 2, 1)\n",
        "        lstm_out = self.bn_lstm(lstm_out)\n",
        "        lstm_out = lstm_out.permute(0, 2, 1)\n",
        "        lstm_out = self.dropout_lstm(lstm_out)\n",
        "        attn_out = self.self_attention(lstm_out)\n",
        "        context_vector = torch.mean(attn_out, dim=1)\n",
        "        x = self.fc1(context_vector)\n",
        "        x = self.bn_fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout_fc(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.bn_fc2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout_fc(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# 1. Load data and class names\n",
        "one_hot_sequences, labels, class_names = load_data(DATA_FOLDER)\n",
        "\n",
        "# 2. Split data into train and test (e.g., 80/20 split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    one_hot_sequences, labels, test_size=0.2, random_state=42, stratify=labels)\n",
        "\n",
        "# 3. Create test dataset and dataloader\n",
        "test_dataset = SequenceDataset(X_test, y_test)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# 4. Setup device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# 5. Number of classes\n",
        "num_classes = len(class_names)\n",
        "\n",
        "# 6. Instantiate the model and load weights\n",
        "model = CNN_LSTM_Model(num_classes=num_classes).to(device)\n",
        "model.load_state_dict(torch.load(FINAL_MODEL_PATH, map_location=device))\n",
        "\n",
        "# 7. Set model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# 8. Run inference on test data and collect predictions and true labels\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for sequences, labels_batch in test_loader:\n",
        "        sequences = sequences.to(device)\n",
        "        labels_batch = labels_batch.to(device)\n",
        "        outputs = model(sequences)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels_batch.cpu().numpy())\n",
        "\n",
        "# 9. Compute confusion matrix\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "# 10. Normalize confusion matrix to percentages (row-wise)\n",
        "cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
        "\n",
        "report = classification_report(all_labels, all_preds, target_names=class_names, digits=4)\n",
        "print(\"Classification Report:\\n\", report)\n",
        "\n",
        "\n",
        "# 11. Visualize confusion matrix with seaborn heatmap\n",
        "\n",
        "\n",
        "\n",
        "plt.figure(figsize=(5, 4))\n",
        "sns.heatmap(cm_percent, annot=True, fmt='.2f', cmap='Blues',\n",
        "                xticklabels=class_names, yticklabels=class_names,\n",
        "                annot_kws={'size': 14})\n",
        "\n",
        "plt.xlabel('Predicted Labels', fontsize=16, labelpad=16)\n",
        "plt.ylabel('True Labels', fontsize=16, labelpad=16)\n",
        "plt.xticks(fontsize=14)\n",
        "plt.yticks(fontsize=14)\n",
        "\n",
        "output_path = '/kaggle/working/Final_confusion_matrix_Plants-Algae.png'\n",
        "plt.savefig(output_path, bbox_inches='tight', dpi=300)\n",
        "plt.show()\n",
        "\n",
        "print(f\"Confusion matrix plot saved to {output_path}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "9p4iFfWzCoAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The code to have the ROC and PR curves\n",
        "\n",
        "# Import required libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.metrics import (classification_report, roc_curve, auc,\n",
        "                            precision_recall_curve, average_precision_score,\n",
        "                            f1_score, matthews_corrcoef)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Parameters\n",
        "SEQ_LENGTH = 300\n",
        "BATCH_SIZE = 64\n",
        "DATA_FOLDER = '/kaggle/input/plant-algae-dataset'\n",
        "FINAL_MODEL_PATH = '/kaggle/input/final-models/Final_Plant-Algae_model.pth'\n",
        "N_SPLITS = 5  # Number of folds for cross-validation\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "# One-hot encoding function\n",
        "def one_hot_encode(sequence, seq_length=SEQ_LENGTH):\n",
        "    nucleotide_map = {'A': [1, 0, 0, 0, 0], 'T': [0, 1, 0, 0, 0],\n",
        "                      'C': [0, 0, 1, 0, 0], 'G': [0, 0, 0, 1, 0],\n",
        "                      'N': [0, 0, 0, 0, 1]}\n",
        "    sequence = sequence.upper().ljust(seq_length, 'N')[:seq_length]\n",
        "    return np.array([nucleotide_map.get(char, [0, 0, 0, 0, 1]) for char in sequence])\n",
        "\n",
        "# Dataset class\n",
        "class SequenceDataset(Dataset):\n",
        "    def __init__(self, sequences, labels):\n",
        "        self.sequences = sequences\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sequence = torch.tensor(self.sequences[idx], dtype=torch.float32)\n",
        "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        return sequence, label\n",
        "\n",
        "# Load data function\n",
        "def load_data(data_folder):\n",
        "    sequences = []\n",
        "    labels = []\n",
        "    class_names = []\n",
        "    for idx, file_name in enumerate(os.listdir(data_folder)):\n",
        "        if file_name.endswith('.csv'):\n",
        "            file_path = os.path.join(data_folder, file_name)\n",
        "            data = pd.read_csv(file_path, header=None)\n",
        "            sequences.extend(data[0].tolist())\n",
        "            labels.extend([idx] * len(data))\n",
        "            class_names.append(os.path.splitext(file_name)[0])\n",
        "    one_hot_sequences = np.array([one_hot_encode(seq) for seq in sequences])\n",
        "    return one_hot_sequences, np.array(labels), class_names\n",
        "\n",
        "# Self-Attention Mechanism\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.attention = nn.MultiheadAttention(embed_dim, num_heads)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(1, 0, 2)\n",
        "        attn_output, _ = self.attention(x, x, x)\n",
        "        return attn_output.permute(1, 0, 2)\n",
        "\n",
        "# Multi-kernel CNN with Residual Connections\n",
        "class MultiKernelCNN(nn.Module):\n",
        "    def __init__(self, input_channels, output_channels, use_multi_kernel=True):\n",
        "        super(MultiKernelCNN, self).__init__()\n",
        "        self.use_multi_kernel = use_multi_kernel\n",
        "        if use_multi_kernel:\n",
        "            self.conv3 = nn.Conv1d(input_channels, output_channels, kernel_size=3, padding=1)\n",
        "            self.conv5 = nn.Conv1d(input_channels, output_channels, kernel_size=5, padding=2)\n",
        "            self.conv7 = nn.Conv1d(input_channels, output_channels, kernel_size=7, padding=3)\n",
        "        else:\n",
        "            self.conv3 = nn.Conv1d(input_channels, output_channels, kernel_size=3, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.residual = nn.Conv1d(input_channels, output_channels * (3 if use_multi_kernel else 1), kernel_size=1)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = self.residual(x)\n",
        "        if self.use_multi_kernel:\n",
        "            x1 = self.relu(self.conv3(x))\n",
        "            x2 = self.relu(self.conv5(x))\n",
        "            x3 = self.relu(self.conv7(x))\n",
        "            x = torch.cat((x1, x2, x3), dim=1)\n",
        "        else:\n",
        "            x = self.relu(self.conv3(x))\n",
        "        x = self.pool(x)\n",
        "        x = self.dropout(x)\n",
        "        if x.shape != residual.shape:\n",
        "            residual = residual[:, :, :x.shape[2]]\n",
        "        x = x + residual\n",
        "        return x\n",
        "\n",
        "# CNN Attention Mechanism\n",
        "class CNN_Attention(nn.Module):\n",
        "    def __init__(self, channel_dim):\n",
        "        super(CNN_Attention, self).__init__()\n",
        "        self.channel_dim = channel_dim\n",
        "        self.query = nn.Conv1d(channel_dim, channel_dim // 8, kernel_size=1)\n",
        "        self.key = nn.Conv1d(channel_dim, channel_dim // 8, kernel_size=1)\n",
        "        self.value = nn.Conv1d(channel_dim, channel_dim, kernel_size=1)\n",
        "        self.gamma = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, channels, seq_len = x.size()\n",
        "        query = self.query(x).view(batch_size, -1, seq_len)\n",
        "        key = self.key(x).view(batch_size, -1, seq_len)\n",
        "        value = self.value(x).view(batch_size, -1, seq_len)\n",
        "        attention_scores = torch.bmm(query.permute(0, 2, 1), key)\n",
        "        attention_scores = F.softmax(attention_scores, dim=-1)\n",
        "        out = torch.bmm(value, attention_scores.permute(0, 2, 1))\n",
        "        out = self.gamma * out + x\n",
        "        return out\n",
        "\n",
        "# CNN-LSTM Hybrid Model with Self-Attention\n",
        "class CNN_LSTM_Model(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(CNN_LSTM_Model, self).__init__()\n",
        "        self.multi_kernel_cnn1 = MultiKernelCNN(input_channels=5, output_channels=128, use_multi_kernel=False)\n",
        "        self.bn1 = nn.BatchNorm1d(128)\n",
        "        self.multi_kernel_cnn2 = MultiKernelCNN(input_channels=128, output_channels=256, use_multi_kernel=True)\n",
        "        self.cnn_attention2 = CNN_Attention(channel_dim=256 * 3)\n",
        "        self.bn2 = nn.BatchNorm1d(256 * 3)\n",
        "        self.residual = nn.Conv1d(in_channels=5, out_channels=256 * 3, kernel_size=1)\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.lstm = nn.LSTM(input_size=256 * 3, hidden_size=256, batch_first=True, bidirectional=True)\n",
        "        self.bn_lstm = nn.BatchNorm1d(256 * 2)\n",
        "        self.self_attention = SelfAttention(embed_dim=256 * 2, num_heads=4)\n",
        "        self.dropout_lstm = nn.Dropout(0.3)\n",
        "        self.fc1 = nn.Linear(256 * 2, 256)\n",
        "        self.bn_fc1 = nn.BatchNorm1d(256)\n",
        "        self.fc2 = nn.Linear(256, 512)\n",
        "        self.bn_fc2 = nn.BatchNorm1d(512)\n",
        "        self.fc3 = nn.Linear(512, num_classes)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout_fc = nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 1)\n",
        "        residual = self.residual(x)\n",
        "        x = self.multi_kernel_cnn1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.multi_kernel_cnn2(x)\n",
        "        x = self.cnn_attention2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.dropout_fc(x)\n",
        "        if x.shape != residual.shape:\n",
        "            residual = residual[:, :, :x.shape[2]]\n",
        "        x = x + residual\n",
        "        x = x.permute(0, 2, 1)\n",
        "        lstm_out, (hn, _) = self.lstm(x)\n",
        "        lstm_out = lstm_out.permute(0, 2, 1)\n",
        "        lstm_out = self.bn_lstm(lstm_out)\n",
        "        lstm_out = lstm_out.permute(0, 2, 1)\n",
        "        lstm_out = self.dropout_lstm(lstm_out)\n",
        "        attn_out = self.self_attention(lstm_out)\n",
        "        context_vector = torch.mean(attn_out, dim=1)\n",
        "        x = self.fc1(context_vector)\n",
        "        x = self.bn_fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout_fc(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.bn_fc2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout_fc(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Main execution\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Load and prepare data\n",
        "    sequences, labels, class_names = load_data(DATA_FOLDER)\n",
        "    print(f\"Sequences shape: {sequences.shape}, Labels shape: {labels.shape}\")\n",
        "    print(f\"Class distribution: {np.bincount(labels)}\")\n",
        "    print(f\"Class names: {class_names}\")\n",
        "\n",
        "    # Initialize variables to store results across folds\n",
        "    all_folds_probs = []\n",
        "    all_folds_labels = []\n",
        "    all_folds_preds = []\n",
        "\n",
        "    # Initialize Stratified K-Fold cross-validator\n",
        "    skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
        "\n",
        "    # Create full dataset\n",
        "    full_dataset = SequenceDataset(sequences, labels)\n",
        "\n",
        "    # Cross-validation loop\n",
        "    for fold, (train_idx, val_idx) in enumerate(skf.split(sequences, labels)):\n",
        "        print(f\"\\n=== Fold {fold + 1}/{N_SPLITS} ===\")\n",
        "\n",
        "        # Create train and validation datasets\n",
        "        train_dataset = Subset(full_dataset, train_idx)\n",
        "        val_dataset = Subset(full_dataset, val_idx)\n",
        "\n",
        "        # Create data loaders\n",
        "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "\n",
        "        # Initialize device and model\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        model = CNN_LSTM_Model(num_classes=len(class_names)).to(device)\n",
        "\n",
        "        # Load final model if available\n",
        "        if os.path.exists(FINAL_MODEL_PATH):\n",
        "            try:\n",
        "                model.load_state_dict(torch.load(FINAL_MODEL_PATH, map_location=device, weights_only=True))\n",
        "                print(f\"Loaded model from: {FINAL_MODEL_PATH}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading model: {e}\")\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        fold_probs = []\n",
        "        fold_labels = []\n",
        "        fold_preds = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for sequences, labels in val_loader:\n",
        "                sequences, labels = sequences.to(device), labels.to(device)\n",
        "                outputs = model(sequences)\n",
        "                probs = F.softmax(outputs, dim=1)\n",
        "\n",
        "                fold_probs.append(probs.cpu().numpy())\n",
        "                fold_labels.append(labels.cpu().numpy())\n",
        "                fold_preds.append(outputs.argmax(dim=1).cpu().numpy())\n",
        "\n",
        "        # Convert lists to numpy arrays\n",
        "        fold_probs = np.concatenate(fold_probs)\n",
        "        fold_labels = np.concatenate(fold_labels)\n",
        "        fold_preds = np.concatenate(fold_preds)\n",
        "\n",
        "        # Store this fold's results\n",
        "        all_folds_probs.append(fold_probs)\n",
        "        all_folds_labels.append(fold_labels)\n",
        "        all_folds_preds.append(fold_preds)\n",
        "\n",
        "        # Print fold results\n",
        "        print(f\"Fold {fold + 1} validation results:\")\n",
        "        print(classification_report(fold_labels, fold_preds, target_names=class_names))\n",
        "\n",
        "    # After all folds, compute and plot average metrics\n",
        "    all_labels = np.concatenate(all_folds_labels)\n",
        "    all_preds = np.concatenate(all_folds_preds)\n",
        "\n",
        "    # Enhanced plotting function with professional settings\n",
        "    def plot_avg_roc_pr_curves(all_folds_probs, all_folds_labels, class_names):\n",
        "        # Set up professional plotting style\n",
        "        plt.style.use('default')\n",
        "        plt.rcParams.update({\n",
        "            'font.family': 'serif',\n",
        "            'font.serif': ['Times New Roman'],\n",
        "            'font.size': 12,\n",
        "            'axes.labelsize': 18,\n",
        "            'axes.titlesize': 16,\n",
        "            'legend.fontsize': 14,\n",
        "            'xtick.labelsize': 14,\n",
        "            'ytick.labelsize': 14,\n",
        "            'figure.dpi': 600,\n",
        "            'savefig.dpi': 600,\n",
        "            'savefig.bbox': 'tight',\n",
        "            'lines.linewidth': 2,\n",
        "            'grid.alpha': 0.3\n",
        "        })\n",
        "\n",
        "        # Concatenate all fold results\n",
        "        y_probs = np.concatenate(all_folds_probs)\n",
        "        y_true = np.concatenate(all_folds_labels)\n",
        "\n",
        "        # For binary classification\n",
        "        y_probs = y_probs[:, 1]  # Positive class probabilities\n",
        "\n",
        "        # Compute metrics\n",
        "        fpr, tpr, _ = roc_curve(y_true, y_probs)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        precision, recall, _ = precision_recall_curve(y_true, y_probs)\n",
        "        average_precision = average_precision_score(y_true, y_probs)\n",
        "\n",
        "        # Compute standard deviations across folds\n",
        "        roc_aucs = []\n",
        "        ap_scores = []\n",
        "\n",
        "        for fold_probs, fold_labels in zip(all_folds_probs, all_folds_labels):\n",
        "            fold_probs = fold_probs[:, 1]\n",
        "            fpr_fold, tpr_fold, _ = roc_curve(fold_labels, fold_probs)\n",
        "            roc_aucs.append(auc(fpr_fold, tpr_fold))\n",
        "            ap_scores.append(average_precision_score(fold_labels, fold_probs))\n",
        "\n",
        "\n",
        "        roc_auc_sem = np.std(roc_aucs, ddof=1) / np.sqrt(len(roc_aucs))\n",
        "        ap_sem = np.std(ap_scores, ddof=1) / np.sqrt(len(ap_scores))\n",
        "\n",
        "\n",
        "        # Create figure with two subplots\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "        # ROC Curve with professional styling\n",
        "        ax1.plot(fpr, tpr, color='#1f77b4', label=f'ROC (AUC = {roc_auc:.3f}  {roc_auc_sem:.3f})')\n",
        "        ax1.plot([0, 1], [0, 1], 'k--', alpha=0.3)\n",
        "        ax1.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05],\n",
        "               xlabel='False Positive Rate',\n",
        "               ylabel='True Positive Rate',\n",
        "               title='ROC Curve')\n",
        "        ax1.grid(True, linestyle='--', alpha=0.3)\n",
        "        ax1.legend(loc='lower right')\n",
        "\n",
        "        # PR Curve with professional styling\n",
        "        ax2.plot(recall, precision, color='#ff7f0e', label=f'PR (AP = {average_precision:.3f}  {ap_sem:.3f})')\n",
        "        ax2.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05],\n",
        "               xlabel='Recall',\n",
        "               ylabel='Precision',\n",
        "               title='Precision-Recall Curve')\n",
        "        ax2.grid(True, linestyle='--', alpha=0.3)\n",
        "        ax2.legend(loc='lower left')\n",
        "\n",
        "        # Adjust layout\n",
        "        plt.tight_layout(pad=3.0)\n",
        "\n",
        "        # Save in multiple formats\n",
        "        output_base = '/kaggle/working/classification_curves_PG_300'\n",
        "        for fmt in ['pdf', 'png', 'svg']:\n",
        "            plt.savefig(f'{output_base}.{fmt}', dpi=600, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "        print(f\"\\nPublication-quality curves saved to:\")\n",
        "        print(f\"- {output_base}.png\")\n",
        "        print(f\"\\nPerformance Metrics:\")\n",
        "        print(f\"ROC AUC: {roc_auc:.4f}  {roc_auc_sem:.4f}\")\n",
        "        print(f\"Average Precision: {average_precision:.4f}  {ap_sem:.4f}\")\n",
        "\n",
        "    # Generate the professional curves\n",
        "    plot_avg_roc_pr_curves(all_folds_probs, all_folds_labels, class_names)\n",
        "\n",
        "    # Print final classification report\n",
        "    print(\"\\n=== Final Classification Report ===\")\n",
        "    print(classification_report(all_labels, all_preds, target_names=class_names))\n",
        "\n",
        "    # Print additional metrics\n",
        "    print(\"\\n=== Additional Metrics ===\")\n",
        "    print(f\"Macro F1-score: {f1_score(all_labels, all_preds, average='macro'):.3f}\")\n",
        "    print(f\"Weighted F1-score: {f1_score(all_labels, all_preds, average='weighted'):.3f}\")\n",
        "    print(f\"Matthews Correlation Coefficient: {matthews_corrcoef(all_labels, all_preds):.3f}\")"
      ],
      "metadata": {
        "id": "G8exxNgCDMlw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The code to have the analysis of attention heatmaps\n",
        "\n",
        "# Import libraries\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, precision_recall_curve, roc_auc_score, f1_score, auc, matthews_corrcoef\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Parameters\n",
        "SEQ_LENGTH = 300\n",
        "BATCH_SIZE = 64\n",
        "DATA_FOLDER = '/kaggle/input/algae-dataset'\n",
        "FINAL_MODEL_PATH = '/kaggle/input/final-models/Final_Algae_model.pth'\n",
        "\n",
        "# One-hot encoding function\n",
        "def one_hot_encode(sequence, seq_length=SEQ_LENGTH):\n",
        "    nucleotide_map = {'A': [1, 0, 0, 0, 0], 'T': [0, 1, 0, 0, 0],\n",
        "                      'C': [0, 0, 1, 0, 0], 'G': [0, 0, 0, 1, 0],\n",
        "                      'N': [0, 0, 0, 0, 1]}\n",
        "    sequence = sequence.upper().ljust(seq_length, 'N')[:seq_length]\n",
        "    return np.array([nucleotide_map.get(char, [0, 0, 0, 0, 1]) for char in sequence])\n",
        "\n",
        "# Dataset class\n",
        "class SequenceDataset(Dataset):\n",
        "    def __init__(self, sequences, labels):\n",
        "        self.sequences = sequences\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sequence = torch.tensor(self.sequences[idx], dtype=torch.float32)\n",
        "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        return sequence, label\n",
        "\n",
        "# Load data function\n",
        "def load_data(data_folder):\n",
        "    sequences = []\n",
        "    labels = []\n",
        "    class_names = []\n",
        "    for idx, file_name in enumerate(os.listdir(data_folder)):\n",
        "        if file_name.endswith('.csv'):\n",
        "            file_path = os.path.join(data_folder, file_name)\n",
        "            data = pd.read_csv(file_path, header=None)\n",
        "            sequences.extend(data[0].tolist())\n",
        "            labels.extend([idx] * len(data))\n",
        "            class_names.append(os.path.splitext(file_name)[0])\n",
        "    one_hot_sequences = np.array([one_hot_encode(seq) for seq in sequences])\n",
        "    return one_hot_sequences, np.array(labels), class_names\n",
        "\n",
        "# Self-Attention Mechanism\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.attention = nn.MultiheadAttention(embed_dim, num_heads)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(1, 0, 2)\n",
        "        attn_output, _ = self.attention(x, x, x)\n",
        "        return attn_output.permute(1, 0, 2)\n",
        "\n",
        "# Multi-kernel CNN with Residual Connections\n",
        "class MultiKernelCNN(nn.Module):\n",
        "    def __init__(self, input_channels, output_channels, use_multi_kernel=True):\n",
        "        super(MultiKernelCNN, self).__init__()\n",
        "        self.use_multi_kernel = use_multi_kernel\n",
        "        if use_multi_kernel:\n",
        "            self.conv3 = nn.Conv1d(input_channels, output_channels, kernel_size=3, padding=1)\n",
        "            self.conv5 = nn.Conv1d(input_channels, output_channels, kernel_size=5, padding=2)\n",
        "            self.conv7 = nn.Conv1d(input_channels, output_channels, kernel_size=7, padding=3)\n",
        "        else:\n",
        "            self.conv3 = nn.Conv1d(input_channels, output_channels, kernel_size=3, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.residual = nn.Conv1d(input_channels, output_channels * (3 if use_multi_kernel else 1), kernel_size=1)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = self.residual(x)\n",
        "        if self.use_multi_kernel:\n",
        "            x1 = self.relu(self.conv3(x))\n",
        "            x2 = self.relu(self.conv5(x))\n",
        "            x3 = self.relu(self.conv7(x))\n",
        "            x = torch.cat((x1, x2, x3), dim=1)\n",
        "        else:\n",
        "            x = self.relu(self.conv3(x))\n",
        "        x = self.pool(x)\n",
        "        x = self.dropout(x)\n",
        "        if x.shape != residual.shape:\n",
        "            residual = residual[:, :, :x.shape[2]]\n",
        "        x = x + residual\n",
        "        return x\n",
        "\n",
        "# CNN Attention Mechanism\n",
        "class CNN_Attention(nn.Module):\n",
        "    def __init__(self, channel_dim):\n",
        "        super(CNN_Attention, self).__init__()\n",
        "        self.channel_dim = channel_dim\n",
        "        self.query = nn.Conv1d(channel_dim, channel_dim // 8, kernel_size=1)\n",
        "        self.key = nn.Conv1d(channel_dim, channel_dim // 8, kernel_size=1)\n",
        "        self.value = nn.Conv1d(channel_dim, channel_dim, kernel_size=1)\n",
        "        self.gamma = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, channels, seq_len = x.size()\n",
        "        query = self.query(x).view(batch_size, -1, seq_len)\n",
        "        key = self.key(x).view(batch_size, -1, seq_len)\n",
        "        value = self.value(x).view(batch_size, -1, seq_len)\n",
        "        attention_scores = torch.bmm(query.permute(0, 2, 1), key)\n",
        "        attention_scores = F.softmax(attention_scores, dim=-1)\n",
        "        out = torch.bmm(value, attention_scores.permute(0, 2, 1))\n",
        "        out = self.gamma * out + x\n",
        "        return out\n",
        "\n",
        "# CNN-LSTM Hybrid Model with Self-Attention\n",
        "class CNN_LSTM_Model(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(CNN_LSTM_Model, self).__init__()\n",
        "        self.multi_kernel_cnn1 = MultiKernelCNN(input_channels=5, output_channels=128, use_multi_kernel=False)\n",
        "        self.bn1 = nn.BatchNorm1d(128)\n",
        "        self.multi_kernel_cnn2 = MultiKernelCNN(input_channels=128, output_channels=256, use_multi_kernel=True)\n",
        "        self.cnn_attention2 = CNN_Attention(channel_dim=256 * 3)\n",
        "        self.bn2 = nn.BatchNorm1d(256 * 3)\n",
        "        self.residual = nn.Conv1d(in_channels=5, out_channels=256 * 3, kernel_size=1)\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.lstm = nn.LSTM(input_size=256 * 3, hidden_size=256, batch_first=True, bidirectional=True)\n",
        "        self.bn_lstm = nn.BatchNorm1d(256 * 2)\n",
        "        self.self_attention = SelfAttention(embed_dim=256 * 2, num_heads=4)\n",
        "        self.dropout_lstm = nn.Dropout(0.3)\n",
        "        self.fc1 = nn.Linear(256 * 2, 256)\n",
        "        self.bn_fc1 = nn.BatchNorm1d(256)\n",
        "        self.fc2 = nn.Linear(256, 512)\n",
        "        self.bn_fc2 = nn.BatchNorm1d(512)\n",
        "        self.fc3 = nn.Linear(512, num_classes)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout_fc = nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 1)\n",
        "        residual = self.residual(x)\n",
        "        x = self.multi_kernel_cnn1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.multi_kernel_cnn2(x)\n",
        "        x = self.cnn_attention2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.dropout_fc(x)\n",
        "        if x.shape != residual.shape:\n",
        "            residual = residual[:, :, :x.shape[2]]\n",
        "        x = x + residual\n",
        "        x = x.permute(0, 2, 1)\n",
        "        lstm_out, (hn, _) = self.lstm(x)\n",
        "        lstm_out = lstm_out.permute(0, 2, 1)\n",
        "        lstm_out = self.bn_lstm(lstm_out)\n",
        "        lstm_out = lstm_out.permute(0, 2, 1)\n",
        "        lstm_out = self.dropout_lstm(lstm_out)\n",
        "        attn_out = self.self_attention(lstm_out)\n",
        "        context_vector = torch.mean(attn_out, dim=1)\n",
        "        x = self.fc1(context_vector)\n",
        "        x = self.bn_fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout_fc(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.bn_fc2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout_fc(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Load and prepare data\n",
        "    sequences, labels, class_names = load_data(DATA_FOLDER)\n",
        "    print(f\"Sequences shape: {sequences.shape}, Labels shape: {labels.shape}\")\n",
        "    print(f\"Class distribution: {np.bincount(labels)}\")\n",
        "\n",
        "    # Split data\n",
        "    _, X_test, _, y_test = train_test_split(\n",
        "        sequences, labels, test_size=0.2, random_state=42, stratify=labels\n",
        "    )\n",
        "\n",
        "    # Create test dataset and loader\n",
        "    test_dataset = SequenceDataset(X_test, y_test)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "\n",
        "    # Initialize device and model\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = CNN_LSTM_Model(num_classes=len(class_names)).to(device)\n",
        "\n",
        "    # Load final model\n",
        "    try:\n",
        "        model.load_state_dict(torch.load(FINAL_MODEL_PATH, map_location=device, weights_only=True))\n",
        "        print(f\"Successfully loaded final model from: {FINAL_MODEL_PATH}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model with weights_only=True: {e}\")\n",
        "        try:\n",
        "            model.load_state_dict(torch.load(FINAL_MODEL_PATH, map_location=device, weights_only=False))\n",
        "            print(\"Successfully loaded with weights_only=False\")\n",
        "        except Exception as e2:\n",
        "            raise RuntimeError(f\"Failed to load model: {e2}\")\n",
        "\n",
        "\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def compute_attention_heatmaps(model, dataloader, device):\n",
        "    \"\"\"\n",
        "    Compute CNN and LSTM attention heatmaps for the given dataloader using the trained model.\n",
        "\n",
        "    Args:\n",
        "        model: Trained model.\n",
        "        dataloader: DataLoader for the test set.\n",
        "        device: Device (e.g., 'cuda' or 'cpu').\n",
        "\n",
        "    Returns:\n",
        "        cnn_attention_weights: List of CNN attention weights for each sequence.\n",
        "        lstm_attention_weights: List of LSTM attention weights for each sequence.\n",
        "        sequences: List of sequences.\n",
        "        labels: List of labels.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    cnn_attention_weights = []\n",
        "    lstm_attention_weights = []\n",
        "    sequences = []\n",
        "    labels = []\n",
        "\n",
        "    for batch_sequences, batch_labels in dataloader:\n",
        "        batch_sequences, batch_labels = batch_sequences.to(device), batch_labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        with torch.no_grad():\n",
        "            # Pass through the first CNN layer\n",
        "            x = batch_sequences.permute(0, 2, 1)\n",
        "            residual = model.residual(x)\n",
        "            x = model.multi_kernel_cnn1(x)\n",
        "\n",
        "            # Pass through the second CNN layer (multi-kernel)\n",
        "            x = model.multi_kernel_cnn2(x)\n",
        "            x = model.pool(x)\n",
        "            x = model.dropout_fc(x)\n",
        "            if x.shape != residual.shape:\n",
        "                residual = residual[:, :, :x.shape[2]]\n",
        "            x = x + residual\n",
        "\n",
        "            # Extract CNN attention weights (feature maps from the second CNN layer)\n",
        "            cnn_attn = x.cpu().numpy()  # Shape: (batch_size, channels, sequence_length)\n",
        "            cnn_attn = cnn_attn.mean(axis=1)  # Average across channels\n",
        "\n",
        "            # Pass through the LSTM layer\n",
        "            x = x.permute(0, 2, 1)\n",
        "            lstm_out, (hn, _) = model.lstm(x)\n",
        "            lstm_out = model.dropout_lstm(lstm_out)\n",
        "\n",
        "            # Pass through the Self-Attention layer\n",
        "            attn_output, attn_weights = model.self_attention.attention(\n",
        "                lstm_out.permute(1, 0, 2),  # (seq_len, batch, embed_dim)\n",
        "                lstm_out.permute(1, 0, 2),\n",
        "                lstm_out.permute(1, 0, 2)\n",
        "            )\n",
        "            attn_output = attn_output.permute(1, 0, 2)  # (batch, seq_len, embed_dim)\n",
        "\n",
        "            # Extract LSTM attention weights\n",
        "            lstm_attn = attn_weights.cpu().numpy()  # Shape: (batch_size, seq_len, seq_len)\n",
        "            lstm_attn = lstm_attn.mean(axis=1)  # Average across sequence positions\n",
        "\n",
        "        # Append results\n",
        "        cnn_attention_weights.extend(cnn_attn)\n",
        "        lstm_attention_weights.extend(lstm_attn)\n",
        "        sequences.extend(batch_sequences.cpu().numpy())\n",
        "        labels.extend(batch_labels.cpu().numpy())\n",
        "\n",
        "    return cnn_attention_weights, lstm_attention_weights, sequences, labels\n",
        "\n",
        "\n",
        "def compute_group_attention_heatmaps(attention_weights, labels, class_names, sequence_length=300):\n",
        "    \"\"\"\n",
        "    Compute group-level attention heatmaps by averaging attention weights for each group.\n",
        "\n",
        "    Args:\n",
        "        attention_weights: List of attention weights.\n",
        "        labels: List of labels.\n",
        "        class_names: List of class names.\n",
        "        sequence_length: Length of the sequences (default: 300 bp).\n",
        "\n",
        "    Returns:\n",
        "        group_attention_heatmaps: Dictionary of average attention heatmaps for each group.\n",
        "    \"\"\"\n",
        "    group_attention_heatmaps = {group: np.zeros(sequence_length) for group in range(len(class_names))}\n",
        "    group_counts = {group: 0 for group in range(len(class_names))}\n",
        "\n",
        "    # Group attention weights by class and sum them\n",
        "    for attn_weights, label in zip(attention_weights, labels):\n",
        "        # Resize attention weights to the original sequence length (300)\n",
        "        attn_weights_resized = F.interpolate(\n",
        "            torch.tensor(attn_weights).unsqueeze(0).unsqueeze(0),  # Add batch and channel dimensions\n",
        "            size=sequence_length,\n",
        "            mode='linear',\n",
        "            align_corners=False\n",
        "        ).squeeze(0).squeeze(0).numpy()  # Remove batch and channel dimensions\n",
        "\n",
        "        # Sum resized attention weights\n",
        "        group_attention_heatmaps[label] += attn_weights_resized\n",
        "        group_counts[label] += 1\n",
        "\n",
        "    # Compute average attention heatmap for each group\n",
        "    for group in group_attention_heatmaps:\n",
        "        group_attention_heatmaps[group] /= group_counts[group]  # Average across sequences\n",
        "\n",
        "    return group_attention_heatmaps\n",
        "\n",
        "\n",
        "\n",
        "def plot_attention_heatmaps(group_attention_heatmaps, class_names, heatmap_type=\"CNN\", save_dir=\"attention_heatmaps\"):\n",
        "    \"\"\"\n",
        "    Plot group-level attention heatmaps and save them with high resolution.\n",
        "    Uses consistent color scale ranges for all CNN maps and all LSTM maps.\n",
        "\n",
        "    Args:\n",
        "        group_attention_heatmaps: Dictionary of average attention heatmaps for each group.\n",
        "        class_names: List of class names.\n",
        "        heatmap_type: Type of attention heatmap (\"CNN\" or \"LSTM\").\n",
        "        save_dir: Directory to save the heatmaps.\n",
        "    \"\"\"\n",
        "    import os\n",
        "\n",
        "    # Create the save directory if it doesn't exist\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # Calculate global min/max for consistent color scaling across all maps of this type\n",
        "    all_values = np.concatenate([heatmap for heatmap in group_attention_heatmaps.values()])\n",
        "    vmin = np.min(all_values)\n",
        "    vmax = np.max(all_values)\n",
        "\n",
        "    for group, group_name in enumerate(class_names):\n",
        "        # Plot Group-Level Attention Heatmap\n",
        "        plt.figure(figsize=(10, 2))\n",
        "        sns.heatmap(\n",
        "            group_attention_heatmaps[group].reshape(1, -1),  # Reshape to 2D for heatmap\n",
        "            cmap=\"viridis\",\n",
        "            cbar=True,\n",
        "            yticklabels=False,  # Disable default y-axis tick labels\n",
        "            vmin=vmin,  # Set consistent min\n",
        "            vmax=vmax   # Set consistent max\n",
        "        )\n",
        "        plt.title(f\"Group-Level {heatmap_type} Attention Heatmap for {group_name} (Test Set)\", fontsize=12)\n",
        "        plt.xlabel(\"Sequence Position (bp)\", fontsize=14, labelpad=13)  # Add fontsize and labelpad\n",
        "        plt.ylabel(\"Average Attention\", fontsize=12, labelpad=13, rotation=90)  # Add y-axis label with rotation\n",
        "\n",
        "        # Set x-axis ticks to match the sequence length (0 to 299 bp)\n",
        "        x_ticks = np.arange(0, 300, step=20)  # Ticks at 0, 50, 100, 150, 200, 250\n",
        "        x_ticks = np.append(x_ticks, 299)  # Add the last position (299)\n",
        "        x_tick_labels = [f\"{-300 + x}\" for x in x_ticks]  # Convert to negative values: -300, -250, ..., -50, -1\n",
        "        x_tick_labels[-1] = \"-1\"  # Replace 0 with -1 for the last tick\n",
        "        plt.xticks(x_ticks, labels=x_tick_labels, fontsize=12, rotation=60, ha='right')  # Rotate x-axis ticks by 30 degrees\n",
        "\n",
        "        # Set y-axis ticks (no labels, since we use plt.ylabel)\n",
        "        plt.yticks(ticks=[0], labels=[\"\"], fontsize=12)  # Empty y-axis tick labels\n",
        "\n",
        "        # Save the heatmap with high resolution\n",
        "        save_path = os.path.join(save_dir, f\"{heatmap_type}_attention_heatmap_{group_name}.png\")\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")  # Save with high resolution (300 DPI)\n",
        "        print(f\"Heatmap saved to {save_path}\")\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "# Create a Dataset for the test set\n",
        "test_dataset = SequenceDataset(X_test, y_test)\n",
        "\n",
        "# Initialize DataLoader for the test set\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# Compute attention weights for the test set\n",
        "cnn_attention_weights, lstm_attention_weights, test_sequences, test_labels = compute_attention_heatmaps(model, test_loader, device)\n",
        "\n",
        "# Compute group-level attention heatmaps\n",
        "cnn_group_attention_heatmaps = compute_group_attention_heatmaps(cnn_attention_weights, test_labels, class_names, sequence_length=300)\n",
        "lstm_group_attention_heatmaps = compute_group_attention_heatmaps(lstm_attention_weights, test_labels, class_names, sequence_length=300)\n",
        "\n",
        "# Plot CNN attention heatmaps\n",
        "plot_attention_heatmaps(cnn_group_attention_heatmaps, class_names, heatmap_type=\"CNN\")\n",
        "\n",
        "# Plot LSTM attention heatmaps\n",
        "plot_attention_heatmaps(lstm_group_attention_heatmaps, class_names, heatmap_type=\"LSTM\")\n"
      ],
      "metadata": {
        "id": "RyLES2ccDq0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " The code to have the Perturbation test analysis\n",
        "\n",
        "# Import libraries\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, precision_recall_curve, roc_auc_score, f1_score, auc, matthews_corrcoef\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Parameters\n",
        "SEQ_LENGTH = 300\n",
        "BATCH_SIZE = 64\n",
        "DATA_FOLDER = '/kaggle/input/plant-dataset'\n",
        "FINAL_MODEL_PATH = '/kaggle/input/final-models/Final_Plant_model.pth'\n",
        "\n",
        "# One-hot encoding function\n",
        "def one_hot_encode(sequence, seq_length=SEQ_LENGTH):\n",
        "    nucleotide_map = {'A': [1, 0, 0, 0, 0], 'T': [0, 1, 0, 0, 0],\n",
        "                      'C': [0, 0, 1, 0, 0], 'G': [0, 0, 0, 1, 0],\n",
        "                      'N': [0, 0, 0, 0, 1]}\n",
        "    sequence = sequence.upper().ljust(seq_length, 'N')[:seq_length]\n",
        "    return np.array([nucleotide_map.get(char, [0, 0, 0, 0, 1]) for char in sequence])\n",
        "\n",
        "# Dataset class\n",
        "class SequenceDataset(Dataset):\n",
        "    def __init__(self, sequences, labels):\n",
        "        self.sequences = sequences\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sequence = torch.tensor(self.sequences[idx], dtype=torch.float32)\n",
        "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        return sequence, label\n",
        "\n",
        "# Load data function\n",
        "def load_data(data_folder):\n",
        "    sequences = []\n",
        "    labels = []\n",
        "    class_names = []\n",
        "    for idx, file_name in enumerate(os.listdir(data_folder)):\n",
        "        if file_name.endswith('.csv'):\n",
        "            file_path = os.path.join(data_folder, file_name)\n",
        "            data = pd.read_csv(file_path, header=None)\n",
        "            sequences.extend(data[0].tolist())\n",
        "            labels.extend([idx] * len(data))\n",
        "            class_names.append(os.path.splitext(file_name)[0])\n",
        "    one_hot_sequences = np.array([one_hot_encode(seq) for seq in sequences])\n",
        "    return one_hot_sequences, np.array(labels), class_names\n",
        "\n",
        "# Self-Attention Mechanism\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.attention = nn.MultiheadAttention(embed_dim, num_heads)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(1, 0, 2)\n",
        "        attn_output, _ = self.attention(x, x, x)\n",
        "        return attn_output.permute(1, 0, 2)\n",
        "\n",
        "# Multi-kernel CNN with Residual Connections\n",
        "class MultiKernelCNN(nn.Module):\n",
        "    def __init__(self, input_channels, output_channels, use_multi_kernel=True):\n",
        "        super(MultiKernelCNN, self).__init__()\n",
        "        self.use_multi_kernel = use_multi_kernel\n",
        "        if use_multi_kernel:\n",
        "            self.conv3 = nn.Conv1d(input_channels, output_channels, kernel_size=3, padding=1)\n",
        "            self.conv5 = nn.Conv1d(input_channels, output_channels, kernel_size=5, padding=2)\n",
        "            self.conv7 = nn.Conv1d(input_channels, output_channels, kernel_size=7, padding=3)\n",
        "        else:\n",
        "            self.conv3 = nn.Conv1d(input_channels, output_channels, kernel_size=3, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.residual = nn.Conv1d(input_channels, output_channels * (3 if use_multi_kernel else 1), kernel_size=1)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = self.residual(x)\n",
        "        if self.use_multi_kernel:\n",
        "            x1 = self.relu(self.conv3(x))\n",
        "            x2 = self.relu(self.conv5(x))\n",
        "            x3 = self.relu(self.conv7(x))\n",
        "            x = torch.cat((x1, x2, x3), dim=1)\n",
        "        else:\n",
        "            x = self.relu(self.conv3(x))\n",
        "        x = self.pool(x)\n",
        "        x = self.dropout(x)\n",
        "        if x.shape != residual.shape:\n",
        "            residual = residual[:, :, :x.shape[2]]\n",
        "        x = x + residual\n",
        "        return x\n",
        "\n",
        "# CNN Attention Mechanism\n",
        "class CNN_Attention(nn.Module):\n",
        "    def __init__(self, channel_dim):\n",
        "        super(CNN_Attention, self).__init__()\n",
        "        self.channel_dim = channel_dim\n",
        "        self.query = nn.Conv1d(channel_dim, channel_dim // 8, kernel_size=1)\n",
        "        self.key = nn.Conv1d(channel_dim, channel_dim // 8, kernel_size=1)\n",
        "        self.value = nn.Conv1d(channel_dim, channel_dim, kernel_size=1)\n",
        "        self.gamma = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, channels, seq_len = x.size()\n",
        "        query = self.query(x).view(batch_size, -1, seq_len)\n",
        "        key = self.key(x).view(batch_size, -1, seq_len)\n",
        "        value = self.value(x).view(batch_size, -1, seq_len)\n",
        "        attention_scores = torch.bmm(query.permute(0, 2, 1), key)\n",
        "        attention_scores = F.softmax(attention_scores, dim=-1)\n",
        "        out = torch.bmm(value, attention_scores.permute(0, 2, 1))\n",
        "        out = self.gamma * out + x\n",
        "        return out\n",
        "\n",
        "# CNN-LSTM Hybrid Model with Self-Attention\n",
        "class CNN_LSTM_Model(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(CNN_LSTM_Model, self).__init__()\n",
        "        self.multi_kernel_cnn1 = MultiKernelCNN(input_channels=5, output_channels=128, use_multi_kernel=False)\n",
        "        self.bn1 = nn.BatchNorm1d(128)\n",
        "        self.multi_kernel_cnn2 = MultiKernelCNN(input_channels=128, output_channels=256, use_multi_kernel=True)\n",
        "        self.cnn_attention2 = CNN_Attention(channel_dim=256 * 3)\n",
        "        self.bn2 = nn.BatchNorm1d(256 * 3)\n",
        "        self.residual = nn.Conv1d(in_channels=5, out_channels=256 * 3, kernel_size=1)\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.lstm = nn.LSTM(input_size=256 * 3, hidden_size=256, batch_first=True, bidirectional=True)\n",
        "        self.bn_lstm = nn.BatchNorm1d(256 * 2)\n",
        "        self.self_attention = SelfAttention(embed_dim=256 * 2, num_heads=4)\n",
        "        self.dropout_lstm = nn.Dropout(0.3)\n",
        "        self.fc1 = nn.Linear(256 * 2, 256)\n",
        "        self.bn_fc1 = nn.BatchNorm1d(256)\n",
        "        self.fc2 = nn.Linear(256, 512)\n",
        "        self.bn_fc2 = nn.BatchNorm1d(512)\n",
        "        self.fc3 = nn.Linear(512, num_classes)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout_fc = nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 1)\n",
        "        residual = self.residual(x)\n",
        "        x = self.multi_kernel_cnn1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.multi_kernel_cnn2(x)\n",
        "        x = self.cnn_attention2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.dropout_fc(x)\n",
        "        if x.shape != residual.shape:\n",
        "            residual = residual[:, :, :x.shape[2]]\n",
        "        x = x + residual\n",
        "        x = x.permute(0, 2, 1)\n",
        "        lstm_out, (hn, _) = self.lstm(x)\n",
        "        lstm_out = lstm_out.permute(0, 2, 1)\n",
        "        lstm_out = self.bn_lstm(lstm_out)\n",
        "        lstm_out = lstm_out.permute(0, 2, 1)\n",
        "        lstm_out = self.dropout_lstm(lstm_out)\n",
        "        attn_out = self.self_attention(lstm_out)\n",
        "        context_vector = torch.mean(attn_out, dim=1)\n",
        "        x = self.fc1(context_vector)\n",
        "        x = self.bn_fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout_fc(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.bn_fc2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout_fc(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Load and prepare data\n",
        "    sequences, labels, class_names = load_data(DATA_FOLDER)\n",
        "    print(f\"Sequences shape: {sequences.shape}, Labels shape: {labels.shape}\")\n",
        "    print(f\"Class distribution: {np.bincount(labels)}\")\n",
        "\n",
        "    # Split data\n",
        "    _, X_test, _, y_test = train_test_split(\n",
        "        sequences, labels, test_size=0.2, random_state=42, stratify=labels\n",
        "    )\n",
        "\n",
        "    # Create test dataset and loader\n",
        "    test_dataset = SequenceDataset(X_test, y_test)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "\n",
        "    # Initialize device and model\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = CNN_LSTM_Model(num_classes=len(class_names)).to(device)\n",
        "\n",
        "    # Load final model\n",
        "    try:\n",
        "        model.load_state_dict(torch.load(FINAL_MODEL_PATH, map_location=device, weights_only=True))\n",
        "        print(f\"Successfully loaded final model from: {FINAL_MODEL_PATH}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model with weights_only=True: {e}\")\n",
        "        try:\n",
        "            model.load_state_dict(torch.load(FINAL_MODEL_PATH, map_location=device, weights_only=False))\n",
        "            print(\"Successfully loaded with weights_only=False\")\n",
        "        except Exception as e2:\n",
        "            raise RuntimeError(f\"Failed to load model: {e2}\")\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import os\n",
        "from torch.profiler import profile, record_function, ProfilerActivity\n",
        "import time\n",
        "import zipfile\n",
        "\n",
        "class PerturbationTester:\n",
        "    def __init__(self, model, dataloader, class_names, sequence_length=300):\n",
        "        self.model = model\n",
        "        self.dataloader = dataloader\n",
        "        self.class_names = class_names\n",
        "        self.sequence_length = sequence_length\n",
        "        self.num_classes = len(class_names)\n",
        "        self.original_predictions_cache = {}\n",
        "\n",
        "        # Define paths for saving and loading checkpoints\n",
        "        self.checkpoint_dir = \"/kaggle/working/perturbation_checkpoints\"  # For saving\n",
        "        self.input_checkpoint_dir = \"/kaggle/input/plant-analysis\"  # For loading\n",
        "\n",
        "        # Create save directory if it doesn't exist\n",
        "        os.makedirs(self.checkpoint_dir, exist_ok=True)\n",
        "\n",
        "        # Check if input directory exists (for resuming)\n",
        "        self.can_resume = os.path.exists(self.input_checkpoint_dir)\n",
        "        if self.can_resume:\n",
        "            print(f\"Found checkpoint directory at {self.input_checkpoint_dir} - can resume\")\n",
        "        else:\n",
        "            print(\"No existing checkpoints found - starting fresh\")\n",
        "\n",
        "    def get_checkpoint_path(self, group, size, pass_num, input_dir=False):\n",
        "        \"\"\"Get path for either saving or loading checkpoints\"\"\"\n",
        "        base_dir = self.input_checkpoint_dir if input_dir else self.checkpoint_dir\n",
        "        return os.path.join(base_dir, f\"group_{group}_size_{size}_pass_{pass_num}.pt\")\n",
        "\n",
        "    def mutate_sequences_vectorized(self, sequences, start_pos, end_pos):\n",
        "        \"\"\"Fully vectorized sequence mutation\"\"\"\n",
        "        mutated = sequences.clone()\n",
        "        batch_size, seq_len, num_channels = sequences.shape\n",
        "\n",
        "        # Pre-compute all possible mutations on same device as input\n",
        "        nucleotides = torch.tensor([\n",
        "            [1, 0, 0, 0, 0], [0, 1, 0, 0, 0],\n",
        "            [0, 0, 1, 0, 0], [0, 0, 0, 1, 0]\n",
        "        ], dtype=torch.float32, device=sequences.device)\n",
        "\n",
        "        # Generate random indices for entire batch\n",
        "        rand_indices = torch.randint(0, 4, (batch_size,), device=sequences.device)\n",
        "\n",
        "        # Create mutation block\n",
        "        mutation_length = end_pos - start_pos\n",
        "        random_nucleotides = nucleotides[rand_indices].unsqueeze(1)  # [batch, 1, 5]\n",
        "        random_nucleotides = random_nucleotides.expand(-1, mutation_length, -1)  # [batch, L, 5]\n",
        "\n",
        "        # Apply mutations\n",
        "        mutated[:, start_pos:end_pos, :] = random_nucleotides\n",
        "        return mutated\n",
        "\n",
        "    def cache_original_predictions(self):\n",
        "        \"\"\"Cache original predictions for all batches\"\"\"\n",
        "        self.model.eval()\n",
        "        self.original_predictions_cache = {}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (sequences, labels) in enumerate(self.dataloader):\n",
        "                sequences = sequences.float()\n",
        "                outputs = self.model(sequences)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                self.original_predictions_cache[batch_idx] = {\n",
        "                    'sequences': sequences,\n",
        "                    'labels': labels,\n",
        "                    'correct': (preds == labels).float()\n",
        "                }\n",
        "\n",
        "    def save_checkpoint(self, results, current_batch, total_batches, mutation_sizes, pass_num):\n",
        "        \"\"\"Save progress every 10% of batches processed\"\"\"\n",
        "        progress = current_batch / total_batches\n",
        "        if progress > 0 and progress % 0.1 < 0.01:  # Every ~10% progress\n",
        "            for group in range(self.num_classes):\n",
        "                for size in mutation_sizes:\n",
        "                    checkpoint_path = self.get_checkpoint_path(group, size, pass_num)\n",
        "                    try:\n",
        "                        torch.save({\n",
        "                            'results': results[group][size],\n",
        "                            'current_batch': current_batch,\n",
        "                            'group_counts': sum(1 for _, labels in self.dataloader\n",
        "                                              if (labels == group).any()),\n",
        "                            'pass_num': pass_num,\n",
        "                            'timestamp': time.time()\n",
        "                        }, checkpoint_path)\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error saving checkpoint {checkpoint_path}: {e}\")\n",
        "            print(f\"Saved checkpoint at {progress*100:.0f}% completion (Pass {pass_num})\")\n",
        "\n",
        "    def load_checkpoint(self, mutation_sizes, pass_num):\n",
        "        \"\"\"Attempt to load existing checkpoints from either input or working directory\"\"\"\n",
        "        results = {group: {size: torch.zeros(self.sequence_length)\n",
        "                         for size in mutation_sizes}\n",
        "                  for group in range(self.num_classes)}\n",
        "        group_counts = torch.zeros(self.num_classes)\n",
        "        last_batch = 0\n",
        "        loaded_from = None\n",
        "\n",
        "        # Try loading from input directory first (previous session)\n",
        "        if self.can_resume:\n",
        "            try:\n",
        "                for group in range(self.num_classes):\n",
        "                    for size in mutation_sizes:\n",
        "                        checkpoint_path = self.get_checkpoint_path(group, size, pass_num, input_dir=True)\n",
        "                        if os.path.exists(checkpoint_path):\n",
        "                            try:\n",
        "                                checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=True)\n",
        "                                if not isinstance(checkpoint, dict) or 'results' not in checkpoint:\n",
        "                                    print(f\"Invalid checkpoint format in {checkpoint_path}\")\n",
        "                                    continue\n",
        "\n",
        "                                results[group][size] = checkpoint['results']\n",
        "                                group_counts[group] = checkpoint.get('group_counts', 0)\n",
        "                                last_batch = max(last_batch, checkpoint.get('current_batch', 0))\n",
        "                                loaded_from = self.input_checkpoint_dir\n",
        "                            except Exception as e:\n",
        "                                print(f\"Error loading checkpoint {checkpoint_path}: {e}\")\n",
        "                                if os.path.exists(checkpoint_path):\n",
        "                                    os.remove(checkpoint_path)\n",
        "\n",
        "                if last_batch > 0:\n",
        "                    print(f\"Resuming from batch {last_batch} (Pass {pass_num}) from input directory\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading from input directory: {e}\")\n",
        "\n",
        "        # If nothing found in input directory, try working directory (current session)\n",
        "        if loaded_from is None:\n",
        "            try:\n",
        "                for group in range(self.num_classes):\n",
        "                    for size in mutation_sizes:\n",
        "                        checkpoint_path = self.get_checkpoint_path(group, size, pass_num)\n",
        "                        if os.path.exists(checkpoint_path):\n",
        "                            try:\n",
        "                                checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=True)\n",
        "                                if not isinstance(checkpoint, dict) or 'results' not in checkpoint:\n",
        "                                    print(f\"Invalid checkpoint format in {checkpoint_path}\")\n",
        "                                    continue\n",
        "\n",
        "                                results[group][size] = checkpoint['results']\n",
        "                                group_counts[group] = checkpoint.get('group_counts', 0)\n",
        "                                last_batch = max(last_batch, checkpoint.get('current_batch', 0))\n",
        "                                loaded_from = self.checkpoint_dir\n",
        "                            except Exception as e:\n",
        "                                print(f\"Error loading checkpoint {checkpoint_path}: {e}\")\n",
        "                                if os.path.exists(checkpoint_path):\n",
        "                                    os.remove(checkpoint_path)\n",
        "\n",
        "                if last_batch > 0:\n",
        "                    print(f\"Resuming from batch {last_batch} (Pass {pass_num}) from working directory\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading from working directory: {e}\")\n",
        "\n",
        "        # Prepare for resuming if we found any checkpoints\n",
        "        if last_batch > 0:\n",
        "            # Create iterator from cache\n",
        "            cache_items = list(self.original_predictions_cache.items())\n",
        "            dataloader_iter = iter(cache_items[last_batch:])\n",
        "            return results, group_counts, dataloader_iter, last_batch\n",
        "\n",
        "        return results, group_counts, None, 0\n",
        "\n",
        "    def transfer_checkpoints_to_input(self):\n",
        "        \"\"\"Transfer checkpoints from working to input directory (for next session)\"\"\"\n",
        "        if not os.path.exists(self.checkpoint_dir):\n",
        "            return False\n",
        "\n",
        "        try:\n",
        "            # Create a zip archive of the checkpoints\n",
        "            zip_path = os.path.join(\"/kaggle/working\", \"perturbation_checkpoints.zip\")\n",
        "            with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "                for root, _, files in os.walk(self.checkpoint_dir):\n",
        "                    for file in files:\n",
        "                        zipf.write(os.path.join(root, file),\n",
        "                                  os.path.relpath(os.path.join(root, file),\n",
        "                                  os.path.join(self.checkpoint_dir)))\n",
        "\n",
        "            print(f\"Checkpoints archived to {zip_path}. Please:\")\n",
        "            print(\"1. Download this zip file\")\n",
        "            print(\"2. Create a new Kaggle dataset with it\")\n",
        "            print(\"3. Add it as input to your next session\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"Error transferring checkpoints: {e}\")\n",
        "            return False\n",
        "\n",
        "    def clear_checkpoints(self, pass_num):\n",
        "        \"\"\"Remove checkpoint files after successful completion\"\"\"\n",
        "        for group in range(self.num_classes):\n",
        "            for size in [1, 4, 8, 12]:  # All possible mutation sizes\n",
        "                checkpoint_path = self.get_checkpoint_path(group, size, pass_num)\n",
        "                if os.path.exists(checkpoint_path):\n",
        "                    try:\n",
        "                        os.remove(checkpoint_path)\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error removing checkpoint {checkpoint_path}: {e}\")\n",
        "\n",
        "    def profile_execution(self):\n",
        "        \"\"\"Profile the execution to identify bottlenecks\"\"\"\n",
        "        with profile(activities=[ProfilerActivity.CPU], record_shapes=True) as prof:\n",
        "            test_results = self.run_perturbation_test(\n",
        "                mutation_sizes=[4],  # Use single size for profiling\n",
        "                window_step=5,\n",
        "                num_workers=2,\n",
        "                refine_iterations=0\n",
        "            )\n",
        "\n",
        "        print(prof.key_averages().table(sort_by=\"cpu_time_total\"))\n",
        "        return test_results\n",
        "\n",
        "    def run_perturbation_test(self, mutation_sizes=[1, 4, 8, 12],\n",
        "                            window_step=3, num_workers=2, refine_iterations=1):\n",
        "        \"\"\"Main perturbation test with all optimizations integrated\"\"\"\n",
        "        start_time = time.time()\n",
        "\n",
        "        # First pass: Coarse screening with window sampling\n",
        "        try:\n",
        "            coarse_results = self._run_pass(\n",
        "                mutation_sizes=mutation_sizes,\n",
        "                window_step=window_step,\n",
        "                num_workers=num_workers,\n",
        "                pass_num=0\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"Error during first pass: {e}\")\n",
        "            print(\"Attempting to recover...\")\n",
        "            # Try to load any partial results\n",
        "            coarse_results, _, _, _ = self.load_checkpoint(mutation_sizes, 0)\n",
        "            if not any(torch.any(r) for group in coarse_results.values() for r in group.values()):\n",
        "                raise RuntimeError(\"Could not recover from error - no valid checkpoints found\")\n",
        "\n",
        "        # Refinement passes for important regions\n",
        "        for iteration in range(refine_iterations):\n",
        "            try:\n",
        "                important_regions = self._identify_important_regions(coarse_results)\n",
        "                refined_results = self._run_pass(\n",
        "                    mutation_sizes=mutation_sizes,\n",
        "                    window_step=1,  # Full resolution for important regions\n",
        "                    num_workers=num_workers,\n",
        "                    focus_regions=important_regions,\n",
        "                    pass_num=iteration+1\n",
        "                )\n",
        "                coarse_results = self._merge_results(coarse_results, refined_results)\n",
        "            except Exception as e:\n",
        "                print(f\"Error during refinement pass {iteration+1}: {e}\")\n",
        "                print(\"Continuing with existing results...\")\n",
        "                break\n",
        "\n",
        "        # Generate final plots\n",
        "        self._plot_results(coarse_results)\n",
        "\n",
        "        # Clear checkpoints after successful completion\n",
        "        for pass_num in range(refine_iterations+1):\n",
        "            self.clear_checkpoints(pass_num)\n",
        "\n",
        "        print(f\"Total execution time: {time.time()-start_time:.2f} seconds\")\n",
        "        return coarse_results\n",
        "\n",
        "    def _run_pass(self, mutation_sizes, window_step, num_workers, focus_regions=None, pass_num=0):\n",
        "        \"\"\"Run a single pass of the perturbation test with checkpointing\"\"\"\n",
        "        if not self.original_predictions_cache:\n",
        "            self.cache_original_predictions()\n",
        "\n",
        "        # Initialize results with error handling\n",
        "        try:\n",
        "            results, group_counts, dataloader_iter, start_batch = self.load_checkpoint(mutation_sizes, pass_num)\n",
        "            if dataloader_iter is None:\n",
        "                dataloader_iter = enumerate(self.original_predictions_cache.items())\n",
        "                start_batch = 0\n",
        "        except Exception as e:\n",
        "            print(f\"Error initializing results: {e}\")\n",
        "            print(\"Starting fresh pass...\")\n",
        "            results = {group: {size: torch.zeros(self.sequence_length)\n",
        "                             for size in mutation_sizes}\n",
        "                      for group in range(self.num_classes)}\n",
        "            group_counts = torch.zeros(self.num_classes)\n",
        "            dataloader_iter = enumerate(self.original_predictions_cache.items())\n",
        "            start_batch = 0\n",
        "\n",
        "        total_batches = len(self.original_predictions_cache)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, batch_data in dataloader_iter:\n",
        "                if isinstance(batch_data, tuple) and len(batch_data) == 2:\n",
        "                    # Handle case where we get (key, value) from items()\n",
        "                    _, batch_data = batch_data\n",
        "\n",
        "                if batch_idx < start_batch:\n",
        "                    continue\n",
        "\n",
        "                sequences = batch_data['sequences']\n",
        "                labels = batch_data['labels']\n",
        "                original_correct = batch_data['correct']\n",
        "\n",
        "                # Get all positions to test\n",
        "                positions = self._get_positions_to_test(window_step, focus_regions)\n",
        "\n",
        "                # Process all mutations for all sizes in parallel\n",
        "                with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
        "                    # Create all mutation tasks\n",
        "                    args_list = [(sequences, labels, original_correct, size, pos)\n",
        "                               for size in mutation_sizes\n",
        "                               for pos in positions]\n",
        "\n",
        "                    # Process in batches for better memory efficiency\n",
        "                    batch_size = 32  # Adjust based on available memory\n",
        "                    for i in range(0, len(args_list), batch_size):\n",
        "                        batch_args = args_list[i:i+batch_size]\n",
        "                        batch_results = list(executor.map(self._process_position, batch_args))\n",
        "\n",
        "                        # Update results\n",
        "                        for (pos, importance_scores), (_, _, _, size, _) in zip(batch_results, batch_args):\n",
        "                            for group in range(self.num_classes):\n",
        "                                group_mask = (labels == group)\n",
        "                                if group_mask.any():\n",
        "                                    results[group][size][pos] += importance_scores[group_mask].sum()\n",
        "\n",
        "                # Update group counts\n",
        "                for group in range(self.num_classes):\n",
        "                    group_counts[group] += (labels == group).sum()\n",
        "\n",
        "                # Save checkpoint periodically\n",
        "                self.save_checkpoint(results, batch_idx+1, total_batches, mutation_sizes, pass_num)\n",
        "\n",
        "        # Normalize results\n",
        "        for group in range(self.num_classes):\n",
        "            if group_counts[group] > 0:\n",
        "                for size in mutation_sizes:\n",
        "                    results[group][size] /= group_counts[group]\n",
        "\n",
        "        return results\n",
        "\n",
        "    def _get_positions_to_test(self, window_step, focus_regions=None):\n",
        "        \"\"\"Determine which positions to test based on focus regions\"\"\"\n",
        "        if focus_regions:\n",
        "            positions = set()\n",
        "            for start, end in focus_regions:\n",
        "                positions.update(range(start, end))\n",
        "            return sorted(positions)\n",
        "        return range(0, self.sequence_length, window_step)\n",
        "\n",
        "    def _process_position(self, args):\n",
        "        \"\"\"Process a single mutation position\"\"\"\n",
        "        sequences, labels, original_correct, size, pos = args\n",
        "        mutated_sequences = self.mutate_sequences_vectorized(\n",
        "            sequences, pos, min(pos + size, self.sequence_length))\n",
        "        outputs = self.model(mutated_sequences)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        mutated_correct = (preds == labels).float()\n",
        "        return pos, original_correct - mutated_correct\n",
        "\n",
        "    def _identify_important_regions(self, results, threshold=0.1):\n",
        "        \"\"\"Vectorized important region identification\"\"\"\n",
        "        all_scores = []\n",
        "\n",
        "        # Collect all scores\n",
        "        for group in range(self.num_classes):\n",
        "            for size in results[group]:\n",
        "                all_scores.append(results[group][size].numpy())\n",
        "\n",
        "        if not all_scores:\n",
        "            return [(0, self.sequence_length)]\n",
        "\n",
        "        # Combine scores and find important regions\n",
        "        combined_scores = np.max(np.stack(all_scores), axis=0)\n",
        "        above_threshold = np.concatenate(([False], combined_scores > threshold, [False]))\n",
        "\n",
        "        # Find region boundaries\n",
        "        diff = np.diff(above_threshold.astype(int))\n",
        "        starts = np.where(diff > 0)[0]\n",
        "        ends = np.where(diff < 0)[0]\n",
        "\n",
        "        # Merge overlapping regions\n",
        "        if len(starts) == 0:\n",
        "            return [(0, self.sequence_length)]\n",
        "\n",
        "        important_regions = list(zip(starts, ends))\n",
        "        important_regions.sort()\n",
        "        merged = [important_regions[0]]\n",
        "\n",
        "        for current in important_regions[1:]:\n",
        "            last = merged[-1]\n",
        "            if current[0] <= last[1]:\n",
        "                merged[-1] = (last[0], max(last[1], current[1]))\n",
        "            else:\n",
        "                merged.append(current)\n",
        "\n",
        "        return merged\n",
        "\n",
        "    def _merge_results(self, coarse, refined):\n",
        "        \"\"\"Merge coarse and refined results\"\"\"\n",
        "        merged = {group: {} for group in range(self.num_classes)}\n",
        "\n",
        "        for group in range(self.num_classes):\n",
        "            for size in coarse[group]:\n",
        "                merged[group][size] = torch.where(\n",
        "                    refined[group][size] != 0,\n",
        "                    refined[group][size],\n",
        "                    coarse[group][size]\n",
        "                )\n",
        "\n",
        "        return merged\n",
        "\n",
        "    def _plot_results(self, results):\n",
        "        \"\"\"Generate plots of the results and save high-resolution versions\"\"\"\n",
        "        # Create directory for saved plots if it doesn't exist\n",
        "        plot_dir = \"/kaggle/working/perturbation_plots\"\n",
        "        os.makedirs(plot_dir, exist_ok=True)\n",
        "\n",
        "        for group, group_name in enumerate(self.class_names):\n",
        "            plt.figure(figsize=(12, 4), dpi=300)  # High DPI for better quality\n",
        "            ax = plt.gca()  # Get current axis\n",
        "\n",
        "            for size in results[group]:\n",
        "                y = results[group][size].numpy()\n",
        "                # Apply smoothing\n",
        "                window_size = min(11, self.sequence_length // 10)\n",
        "                if window_size > 1:\n",
        "                    y = np.convolve(y, np.ones(window_size)/window_size, mode='same')\n",
        "                plt.plot(range(self.sequence_length), y, label=f'Mutation Size: {size} nt', linewidth=2)\n",
        "\n",
        "            plt.title(f\"Importance Score Over Sequence for {group_name}\", fontsize=16, pad=20)\n",
        "            plt.xlabel(\"Sequence Position (bp)\", fontsize=18, labelpad=15)\n",
        "            plt.ylabel(\"Importance Score\", fontsize=18, labelpad=13)\n",
        "\n",
        "            # Set y-axis limits and tick fontsize\n",
        "            plt.ylim(-0.09, 2.00)\n",
        "            plt.yticks(fontsize=14)\n",
        "\n",
        "            # # Modified x-ticks to show negative values\n",
        "            x_ticks = np.arange(0, self.sequence_length + 1, step=20)  # Ticks at 0, 20, 40, ..., 300\n",
        "            x_tick_labels = [f\"{-self.sequence_length + x}\" for x in x_ticks]  # Convert to negative values\n",
        "            x_tick_labels[-1] = \"-1\"  # Replace 0 with -1 for the last tick\n",
        "\n",
        "            plt.xticks(x_ticks, labels=x_tick_labels, fontsize=16, rotation=30, ha='right')\n",
        "\n",
        "            # Remove top and right spines\n",
        "            ax.spines['top'].set_visible(False)\n",
        "            ax.spines['right'].set_visible(False)\n",
        "\n",
        "            # Adjust legend\n",
        "            plt.legend(fontsize=14, framealpha=0.9, loc='upper left', bbox_to_anchor=(0.02, 0.98))\n",
        "            # Set x-axis limits to ensure all ticks are visible\n",
        "            plt.xlim(0, self.sequence_length)\n",
        "\n",
        "            # Tight layout to prevent label cutoff\n",
        "            plt.tight_layout()\n",
        "\n",
        "            # Save high-resolution version\n",
        "            plot_path = os.path.join(plot_dir, f\"{group_name}_importance_plot.png\")\n",
        "            plt.savefig(plot_path, dpi=300, bbox_inches='tight', format='png')\n",
        "            print(f\"Saved high-resolution plot to: {plot_path}\")\n",
        "\n",
        "            # Show plot\n",
        "            plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# Usage Example:\n",
        "tester = PerturbationTester(model, test_loader, class_names)\n",
        "\n",
        "try:\n",
        "    # Option 1: Run with profiling first to identify bottlenecks\n",
        "    # profiled_results = tester.profile_execution()\n",
        "\n",
        "    # Option 2: Run full optimized test\n",
        "    final_results = tester.run_perturbation_test(\n",
        "        mutation_sizes=[1, 4, 8, 12],\n",
        "        window_step=3,          # Start with coarse sampling\n",
        "        num_workers=4,         # Adjust based on the CPU\n",
        "        refine_iterations=1    # Add refinement passes\n",
        "    )\n",
        "\n",
        "    # Optionally transfer checkpoints for next session\n",
        "    tester.transfer_checkpoints_to_input()\n",
        "except Exception as e:\n",
        "    print(f\"Run interrupted: {e}\")\n",
        "    print(\"Progress has been saved. When you restart, it will resume from last checkpoint.\")"
      ],
      "metadata": {
        "id": "BYj_nZ61DwwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " The code to have the Group saliency map analysis\n",
        "\n",
        "# Import libraries\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, precision_recall_curve, roc_auc_score, f1_score, auc, matthews_corrcoef\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Parameters\n",
        "SEQ_LENGTH = 300\n",
        "BATCH_SIZE = 64\n",
        "DATA_FOLDER = '/kaggle/input/algae-dataset'\n",
        "FINAL_MODEL_PATH = '/kaggle/input/final-models/Final_Algae_model.pth'\n",
        "\n",
        "# One-hot encoding function\n",
        "def one_hot_encode(sequence, seq_length=SEQ_LENGTH):\n",
        "    nucleotide_map = {'A': [1, 0, 0, 0, 0], 'T': [0, 1, 0, 0, 0],\n",
        "                      'C': [0, 0, 1, 0, 0], 'G': [0, 0, 0, 1, 0],\n",
        "                      'N': [0, 0, 0, 0, 1]}\n",
        "    sequence = sequence.upper().ljust(seq_length, 'N')[:seq_length]\n",
        "    return np.array([nucleotide_map.get(char, [0, 0, 0, 0, 1]) for char in sequence])\n",
        "\n",
        "# Dataset class\n",
        "class SequenceDataset(Dataset):\n",
        "    def __init__(self, sequences, labels):\n",
        "        self.sequences = sequences\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sequence = torch.tensor(self.sequences[idx], dtype=torch.float32)\n",
        "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        return sequence, label\n",
        "\n",
        "# Load data function\n",
        "def load_data(data_folder):\n",
        "    sequences = []\n",
        "    labels = []\n",
        "    class_names = []\n",
        "    for idx, file_name in enumerate(os.listdir(data_folder)):\n",
        "        if file_name.endswith('.csv'):\n",
        "            file_path = os.path.join(data_folder, file_name)\n",
        "            data = pd.read_csv(file_path, header=None)\n",
        "            sequences.extend(data[0].tolist())\n",
        "            labels.extend([idx] * len(data))\n",
        "            class_names.append(os.path.splitext(file_name)[0])\n",
        "    one_hot_sequences = np.array([one_hot_encode(seq) for seq in sequences])\n",
        "    return one_hot_sequences, np.array(labels), class_names\n",
        "\n",
        "# Self-Attention Mechanism\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.attention = nn.MultiheadAttention(embed_dim, num_heads)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(1, 0, 2)\n",
        "        attn_output, _ = self.attention(x, x, x)\n",
        "        return attn_output.permute(1, 0, 2)\n",
        "\n",
        "# Multi-kernel CNN with Residual Connections\n",
        "class MultiKernelCNN(nn.Module):\n",
        "    def __init__(self, input_channels, output_channels, use_multi_kernel=True):\n",
        "        super(MultiKernelCNN, self).__init__()\n",
        "        self.use_multi_kernel = use_multi_kernel\n",
        "        if use_multi_kernel:\n",
        "            self.conv3 = nn.Conv1d(input_channels, output_channels, kernel_size=3, padding=1)\n",
        "            self.conv5 = nn.Conv1d(input_channels, output_channels, kernel_size=5, padding=2)\n",
        "            self.conv7 = nn.Conv1d(input_channels, output_channels, kernel_size=7, padding=3)\n",
        "        else:\n",
        "            self.conv3 = nn.Conv1d(input_channels, output_channels, kernel_size=3, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.residual = nn.Conv1d(input_channels, output_channels * (3 if use_multi_kernel else 1), kernel_size=1)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = self.residual(x)\n",
        "        if self.use_multi_kernel:\n",
        "            x1 = self.relu(self.conv3(x))\n",
        "            x2 = self.relu(self.conv5(x))\n",
        "            x3 = self.relu(self.conv7(x))\n",
        "            x = torch.cat((x1, x2, x3), dim=1)\n",
        "        else:\n",
        "            x = self.relu(self.conv3(x))\n",
        "        x = self.pool(x)\n",
        "        x = self.dropout(x)\n",
        "        if x.shape != residual.shape:\n",
        "            residual = residual[:, :, :x.shape[2]]\n",
        "        x = x + residual\n",
        "        return x\n",
        "\n",
        "# CNN Attention Mechanism\n",
        "class CNN_Attention(nn.Module):\n",
        "    def __init__(self, channel_dim):\n",
        "        super(CNN_Attention, self).__init__()\n",
        "        self.channel_dim = channel_dim\n",
        "        self.query = nn.Conv1d(channel_dim, channel_dim // 8, kernel_size=1)\n",
        "        self.key = nn.Conv1d(channel_dim, channel_dim // 8, kernel_size=1)\n",
        "        self.value = nn.Conv1d(channel_dim, channel_dim, kernel_size=1)\n",
        "        self.gamma = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, channels, seq_len = x.size()\n",
        "        query = self.query(x).view(batch_size, -1, seq_len)\n",
        "        key = self.key(x).view(batch_size, -1, seq_len)\n",
        "        value = self.value(x).view(batch_size, -1, seq_len)\n",
        "        attention_scores = torch.bmm(query.permute(0, 2, 1), key)\n",
        "        attention_scores = F.softmax(attention_scores, dim=-1)\n",
        "        out = torch.bmm(value, attention_scores.permute(0, 2, 1))\n",
        "        out = self.gamma * out + x\n",
        "        return out\n",
        "\n",
        "# CNN-LSTM Hybrid Model with Self-Attention\n",
        "class CNN_LSTM_Model(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(CNN_LSTM_Model, self).__init__()\n",
        "        self.multi_kernel_cnn1 = MultiKernelCNN(input_channels=5, output_channels=128, use_multi_kernel=False)\n",
        "        self.bn1 = nn.BatchNorm1d(128)\n",
        "        self.multi_kernel_cnn2 = MultiKernelCNN(input_channels=128, output_channels=256, use_multi_kernel=True)\n",
        "        self.cnn_attention2 = CNN_Attention(channel_dim=256 * 3)\n",
        "        self.bn2 = nn.BatchNorm1d(256 * 3)\n",
        "        self.residual = nn.Conv1d(in_channels=5, out_channels=256 * 3, kernel_size=1)\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.lstm = nn.LSTM(input_size=256 * 3, hidden_size=256, batch_first=True, bidirectional=True)\n",
        "        self.bn_lstm = nn.BatchNorm1d(256 * 2)\n",
        "        self.self_attention = SelfAttention(embed_dim=256 * 2, num_heads=4)\n",
        "        self.dropout_lstm = nn.Dropout(0.3)\n",
        "        self.fc1 = nn.Linear(256 * 2, 256)\n",
        "        self.bn_fc1 = nn.BatchNorm1d(256)\n",
        "        self.fc2 = nn.Linear(256, 512)\n",
        "        self.bn_fc2 = nn.BatchNorm1d(512)\n",
        "        self.fc3 = nn.Linear(512, num_classes)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout_fc = nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 1)\n",
        "        residual = self.residual(x)\n",
        "        x = self.multi_kernel_cnn1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.multi_kernel_cnn2(x)\n",
        "        x = self.cnn_attention2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.dropout_fc(x)\n",
        "        if x.shape != residual.shape:\n",
        "            residual = residual[:, :, :x.shape[2]]\n",
        "        x = x + residual\n",
        "        x = x.permute(0, 2, 1)\n",
        "        lstm_out, (hn, _) = self.lstm(x)\n",
        "        lstm_out = lstm_out.permute(0, 2, 1)\n",
        "        lstm_out = self.bn_lstm(lstm_out)\n",
        "        lstm_out = lstm_out.permute(0, 2, 1)\n",
        "        lstm_out = self.dropout_lstm(lstm_out)\n",
        "        attn_out = self.self_attention(lstm_out)\n",
        "        context_vector = torch.mean(attn_out, dim=1)\n",
        "        x = self.fc1(context_vector)\n",
        "        x = self.bn_fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout_fc(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.bn_fc2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout_fc(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Load and prepare data\n",
        "    sequences, labels, class_names = load_data(DATA_FOLDER)\n",
        "    print(f\"Sequences shape: {sequences.shape}, Labels shape: {labels.shape}\")\n",
        "    print(f\"Class distribution: {np.bincount(labels)}\")\n",
        "\n",
        "    # Split data\n",
        "    _, X_test, _, y_test = train_test_split(\n",
        "        sequences, labels, test_size=0.2, random_state=42, stratify=labels\n",
        "    )\n",
        "\n",
        "    # Create test dataset and loader\n",
        "    test_dataset = SequenceDataset(X_test, y_test)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "\n",
        "    # Initialize device and model\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = CNN_LSTM_Model(num_classes=len(class_names)).to(device)\n",
        "\n",
        "    # Load final model\n",
        "    try:\n",
        "        model.load_state_dict(torch.load(FINAL_MODEL_PATH, map_location=device, weights_only=True))\n",
        "        print(f\"Successfully loaded final model from: {FINAL_MODEL_PATH}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model with weights_only=True: {e}\")\n",
        "        try:\n",
        "            model.load_state_dict(torch.load(FINAL_MODEL_PATH, map_location=device, weights_only=False))\n",
        "            print(\"Successfully loaded with weights_only=False\")\n",
        "        except Exception as e2:\n",
        "            raise RuntimeError(f\"Failed to load model: {e2}\")\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Function to compute saliency maps\n",
        "def compute_saliency_maps(model, dataloader, device):\n",
        "    \"\"\"\n",
        "    Compute saliency maps for the given dataloader using the trained model.\n",
        "\n",
        "    Args:\n",
        "        model: Trained model.\n",
        "        dataloader: DataLoader for the test set.\n",
        "        device: Device (e.g., 'cuda' or 'cpu').\n",
        "\n",
        "    Returns:\n",
        "        saliency_maps: List of saliency maps for each sequence.\n",
        "        sequences: List of sequences.\n",
        "        labels: List of labels.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    saliency_maps = []\n",
        "    sequences = []\n",
        "    labels = []\n",
        "\n",
        "    for batch_sequences, batch_labels in dataloader:\n",
        "        batch_sequences, batch_labels = batch_sequences.to(device), batch_labels.to(device)\n",
        "        batch_sequences.requires_grad_()  # Enable gradient computation for inputs\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(batch_sequences)  # Outputs should be of shape (batch_size, num_classes)\n",
        "\n",
        "        # Use the predicted class for saliency\n",
        "        _, preds = torch.max(outputs, dim=1)  # Get predicted class indices\n",
        "\n",
        "        # Select the output corresponding to the predicted class\n",
        "        selected_outputs = outputs[torch.arange(outputs.size(0)), preds]  # Shape: (batch_size,)\n",
        "\n",
        "        # Compute gradients of the selected outputs with respect to the input\n",
        "        selected_outputs.sum().backward()  # Sum to get a scalar for backward pass\n",
        "        gradients = batch_sequences.grad.data.abs().cpu().numpy()  # Take absolute value of gradients\n",
        "\n",
        "        # Append results\n",
        "        saliency_maps.extend(gradients)\n",
        "        sequences.extend(batch_sequences.detach().cpu().numpy())\n",
        "        labels.extend(batch_labels.cpu().numpy())\n",
        "\n",
        "    return saliency_maps, sequences, labels\n",
        "# Function to compute group-level saliency maps\n",
        "def compute_group_saliency_maps(saliency_maps, labels, class_names, sequence_length=300):\n",
        "    \"\"\"\n",
        "    Compute group-level saliency maps by averaging saliency maps for each group.\n",
        "\n",
        "    Args:\n",
        "        saliency_maps: List of saliency maps.\n",
        "        labels: List of labels.\n",
        "        class_names: List of class names.\n",
        "        sequence_length: Length of the sequences (default: 300 bp).\n",
        "\n",
        "    Returns:\n",
        "        group_saliency_maps: Dictionary of average saliency maps for each group.\n",
        "    \"\"\"\n",
        "    group_saliency_maps = {group: np.zeros(sequence_length) for group in range(len(class_names))}\n",
        "    group_counts = {group: 0 for group in range(len(class_names))}\n",
        "\n",
        "    # Group saliency maps by class and sum them\n",
        "    for saliency_map, label in zip(saliency_maps, labels):\n",
        "        # Sum saliency values for each position (0 to 299 bp)\n",
        "        group_saliency_maps[label] += saliency_map.sum(axis=1)  # Sum across channels (e.g., one-hot encoding)\n",
        "        group_counts[label] += 1\n",
        "\n",
        "    # Compute average saliency map for each group\n",
        "    for group in group_saliency_maps:\n",
        "        group_saliency_maps[group] /= group_counts[group]  # Average across sequences\n",
        "\n",
        "    return group_saliency_maps\n",
        "\n",
        "\n",
        "# Function to plot group-level saliency maps\n",
        "def plot_group_saliency_maps(group_saliency_maps, class_names, sequence_length=300):\n",
        "    \"\"\"\n",
        "    Plot group-level saliency maps with enhanced formatting and consistent y-axis (0.0-0.8).\n",
        "    Saves each map separately with high resolution (600 DPI).\n",
        "\n",
        "    Args:\n",
        "        group_saliency_maps: Dictionary of average saliency maps for each group\n",
        "        class_names: List of class names\n",
        "        sequence_length: Length of the sequences (default: 300 bp)\n",
        "    \"\"\"\n",
        "    # Set consistent y-axis parameters\n",
        "    y_min, y_max = 0.0, 0.8\n",
        "    y_ticks = np.arange(y_min, y_max + 0.01, 0.2)  # 0.0, 0.2, 0.4, 0.6, 0.8\n",
        "\n",
        "    for group, group_name in enumerate(class_names):\n",
        "        # Create figure with white background\n",
        "        fig, ax = plt.subplots(figsize=(12, 4), facecolor='white')\n",
        "\n",
        "        # Plot Group-Level Saliency Map\n",
        "        ax.plot(np.arange(sequence_length), group_saliency_maps[group],\n",
        "                color='red', linewidth=2.0)\n",
        "\n",
        "        # Title and labels with improved formatting\n",
        "        ax.set_title(f\"Group-Level Saliency Map for {group_name}\",\n",
        "                    fontsize=14, pad=15, fontweight='bold')\n",
        "        ax.set_xlabel(\"Sequence Position (bp)\",\n",
        "                     fontsize=18, labelpad=15)\n",
        "        ax.set_ylabel(\"Average Saliency\",\n",
        "                     fontsize=16, labelpad=15)\n",
        "\n",
        "        # Set y-axis parameters\n",
        "        ax.set_ylim(y_min, y_max)\n",
        "        ax.set_yticks(y_ticks)\n",
        "        ax.tick_params(axis='y', labelsize=14)\n",
        "\n",
        "        # Customize x-axis ticks\n",
        "        x_ticks = np.arange(0, sequence_length + 1, step=50)\n",
        "        x_tick_labels = [f\"{-sequence_length + x}\" for x in x_ticks]\n",
        "        x_tick_labels[-1] = \"-1\"\n",
        "        ax.set_xticks(x_ticks)\n",
        "        ax.set_xticklabels(x_tick_labels, fontsize=14, rotation=45, ha='right')\n",
        "\n",
        "\n",
        "\n",
        "        # Remove top and right spines\n",
        "        ax.spines['top'].set_visible(False)\n",
        "        ax.spines['right'].set_visible(False)\n",
        "\n",
        "        # Adjust layout\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Save each plot separately with high resolution\n",
        "        output_filename = f\"Saliency_Map_{group_name.replace(' ', '_')}.png\"\n",
        "        plt.savefig(output_filename, dpi=600, bbox_inches='tight', facecolor=fig.get_facecolor())\n",
        "        print(f\"Saved high-resolution saliency map for {group_name} as {output_filename}\")\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# Create a Dataset for the test set\n",
        "test_dataset = SequenceDataset(X_test, y_test)\n",
        "\n",
        "# Initialize DataLoader for the test set\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# Compute saliency maps for the test set\n",
        "saliency_maps, test_sequences, test_labels = compute_saliency_maps(model, test_loader, device)\n",
        "\n",
        "# Compute group-level saliency maps\n",
        "group_saliency_maps = compute_group_saliency_maps(saliency_maps, test_labels, class_names, sequence_length=300)\n",
        "\n",
        "# Plot group-level saliency maps\n",
        "plot_group_saliency_maps(group_saliency_maps, class_names, sequence_length=300)\n",
        "\n",
        "\n",
        "from Bio import motifs\n",
        "from Bio.Seq import Seq\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def identify_important_regions(group_saliency_maps, class_names, window_size=20, top_n=5):\n",
        "    \"\"\"\n",
        "    Identify important regions by finding peaks in saliency maps.\n",
        "\n",
        "    Args:\n",
        "        group_saliency_maps: Dictionary of saliency maps per group\n",
        "        class_names: List of class names\n",
        "        window_size: Size of region to consider around each peak\n",
        "        top_n: Number of top regions to return\n",
        "\n",
        "    Returns:\n",
        "        Dictionary of important regions for each group\n",
        "    \"\"\"\n",
        "    important_regions = {}\n",
        "\n",
        "    for group, group_name in enumerate(class_names):\n",
        "        saliency = group_saliency_maps[group]\n",
        "\n",
        "        # Find peaks (local maxima)\n",
        "        peaks = []\n",
        "        for i in range(1, len(saliency)-1):\n",
        "            if saliency[i] > saliency[i-1] and saliency[i] > saliency[i+1]:\n",
        "                peaks.append(i)\n",
        "\n",
        "        # Sort peaks by saliency score\n",
        "        peaks.sort(key=lambda x: saliency[x], reverse=True)\n",
        "\n",
        "        # Get top N regions\n",
        "        regions = []\n",
        "        for peak in peaks[:top_n]:\n",
        "            start = max(0, peak - window_size//2)\n",
        "            end = min(len(saliency), peak + window_size//2)\n",
        "            regions.append((start, end, saliency[peak]))\n",
        "\n",
        "        important_regions[group_name] = regions\n",
        "\n",
        "    return important_regions\n",
        "\n",
        "def get_nucleotide_color(letter, alpha=1.0):\n",
        "    \"\"\"Return colors for nucleotides.\"\"\"\n",
        "    colors = {\n",
        "        'A': (0.0, 0.6, 0.0, alpha),    # Green for A\n",
        "        'T': (0.8, 0.0, 0.0, alpha),    # Red for T\n",
        "        'C': (0.0, 0.0, 0.8, alpha),    # Blue for C\n",
        "        'G': (0.9, 0.6, 0.0, alpha),    # Orange for G\n",
        "        'N': (0.5, 0.5, 0.5, alpha)     # Gray for N\n",
        "    }\n",
        "    return colors.get(letter.upper(), (0.0, 0.0, 0.0, alpha))\n",
        "\n",
        "def create_sequence_logos(test_sequences, test_labels, class_names, important_regions):\n",
        "    \"\"\"\n",
        "    Create sequence logos for important regions in each group using Bio.motifs.\n",
        "\n",
        "    Args:\n",
        "        test_sequences: List of all test sequences (one-hot encoded)\n",
        "        test_labels: List of corresponding labels\n",
        "        class_names: List of class names\n",
        "        important_regions: Dictionary of important regions per group\n",
        "    \"\"\"\n",
        "    # Convert one-hot encoded sequences back to nucleotide sequences\n",
        "    nucleotide_map = {0: 'A', 1: 'T', 2: 'C', 3: 'G', 4: 'N'}\n",
        "    seq_strings = []\n",
        "    for seq in test_sequences:\n",
        "        # Convert one-hot to nucleotides\n",
        "        seq_str = ''.join([nucleotide_map[np.argmax(pos)] for pos in seq])\n",
        "        seq_strings.append(seq_str)\n",
        "\n",
        "    # For each group and its important regions\n",
        "    for group_name, regions in important_regions.items():\n",
        "        group_idx = class_names.index(group_name)\n",
        "        group_seqs = [seq for seq, label in zip(seq_strings, test_labels) if label == group_idx]\n",
        "\n",
        "        for i, (start, end, score) in enumerate(regions):\n",
        "            # Extract region sequences for this group\n",
        "            region_seqs = [seq[start:end] for seq in group_seqs if len(seq) >= end]\n",
        "\n",
        "            if not region_seqs:\n",
        "                print(f\"No sequences long enough for {group_name} region {i+1} ({start}-{end})\")\n",
        "                continue\n",
        "\n",
        "            # Create a motif from these sequences\n",
        "            try:\n",
        "                instances = [Seq(seq) for seq in region_seqs]\n",
        "                m = motifs.create(instances)\n",
        "\n",
        "                # Create a figure\n",
        "                plt.figure(figsize=(10, 3))\n",
        "\n",
        "                # Create a position weight matrix\n",
        "                pwm = m.counts.normalize(pseudocounts=0.5)\n",
        "\n",
        "                # Calculate information content\n",
        "                ic = []\n",
        "                for position in range(len(m)):\n",
        "                    freq = pwm[position]\n",
        "                    entropy = -sum(freq[base] * np.log2(freq[base]) if freq[base] > 0 else 0 for base in 'ATCG')\n",
        "                    ic.append(2 - entropy)\n",
        "\n",
        "                # Plot the sequence logo\n",
        "                for position in range(len(ic)):\n",
        "                    # Sort nucleotides by frequency\n",
        "                    freq = pwm[position]\n",
        "                    sorted_bases = sorted([(base, freq[base]) for base in 'ATCG'],\n",
        "                                         key=lambda x: x[1], reverse=True)\n",
        "\n",
        "                    yshift = 0\n",
        "                    for base, f in sorted_bases:\n",
        "                        if f > 0.1:  # Only show nucleotides with >10% frequency\n",
        "                            height = f * ic[position]\n",
        "                            plt.text(position + 0.5, yshift + height/2, base,\n",
        "                                    ha='center', va='center', fontsize=12,\n",
        "                                    color=get_nucleotide_color(base))\n",
        "                            plt.fill_between([position, position+1],\n",
        "                                           [yshift, yshift],\n",
        "                                           [yshift + height, yshift + height],\n",
        "                                           color=get_nucleotide_color(base, alpha=0.3))\n",
        "                            yshift += height\n",
        "\n",
        "                # Style the plot\n",
        "                plt.xlim(0, end-start)\n",
        "                plt.ylim(0, 2)  # Max information content is 2 bits\n",
        "                plt.xticks(np.arange(0.5, end-start+0.5, 1),\n",
        "                          labels=np.arange(start, end, 1), rotation=45)\n",
        "                plt.xlabel('Position (bp)')\n",
        "                plt.ylabel('Information (bits)')\n",
        "                plt.title(f'Sequence Logo for {group_name}\\nRegion {i+1} (bp {start}-{end}), Saliency score: {score:.3f}')\n",
        "                plt.grid(False)\n",
        "\n",
        "                # Save the plot\n",
        "                filename = f\"Sequence_Logo_{group_name.replace(' ', '_')}_Region_{i+1}.png\"\n",
        "                plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
        "                plt.close()\n",
        "                print(f\"Saved sequence logo for {group_name} region {i+1} as {filename}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error creating logo for {group_name} region {i+1}: {str(e)}\")\n",
        "                plt.close()\n",
        "\n",
        "# Identify important regions from saliency maps\n",
        "important_regions = identify_important_regions(group_saliency_maps, class_names)\n",
        "\n",
        "# Create sequence logos for important regions\n",
        "create_sequence_logos(test_sequences, test_labels, class_names, important_regions)"
      ],
      "metadata": {
        "id": "tTqPBzF3EcMt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}